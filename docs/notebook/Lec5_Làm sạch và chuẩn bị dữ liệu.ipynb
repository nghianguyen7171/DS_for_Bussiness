{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a35277-2812-41cc-a00e-c99ade49156f",
   "metadata": {},
   "source": [
    "# ğŸ“Š LÃ m sáº¡ch vÃ  chuáº©n bá»‹ dá»¯ liá»‡u trong Khoa há»c Dá»¯ liá»‡u\n",
    "\n",
    "## ğŸ¯ Má»¥c tiÃªu há»c táº­p\n",
    "\n",
    "Sau khi hoÃ n thÃ nh bÃ i há»c nÃ y, báº¡n sáº½ cÃ³ thá»ƒ:\n",
    "\n",
    "âœ… **Hiá»ƒu vÃ  xá»­ lÃ½ dá»¯ liá»‡u thiáº¿u** trong cÃ¡c bá»™ dá»¯ liá»‡u kinh táº¿  \n",
    "âœ… **PhÃ¡t hiá»‡n vÃ  loáº¡i bá» dá»¯ liá»‡u trÃ¹ng láº·p** trong kháº£o sÃ¡t khÃ¡ch hÃ ng  \n",
    "âœ… **Chuáº©n hÃ³a vÃ  biáº¿n Ä‘á»•i dá»¯ liá»‡u** Ä‘á»ƒ phÃ¹ há»£p vá»›i phÃ¢n tÃ­ch  \n",
    "âœ… **Xá»­ lÃ½ dá»¯ liá»‡u chuá»—i kÃ½ tá»±** tá»« cÃ¡c nguá»“n khÃ¡c nhau  \n",
    "âœ… **MÃ£ hÃ³a dá»¯ liá»‡u phÃ¢n loáº¡i** cho machine learning  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Lá»™ trÃ¬nh há»c táº­p\n",
    "\n",
    "1. **ğŸ” Xá»­ lÃ½ dá»¯ liá»‡u thiáº¿u** - Táº¡i sao dá»¯ liá»‡u bá»‹ thiáº¿u vÃ  cÃ¡ch xá»­ lÃ½\n",
    "2. **ğŸ”„ Xá»­ lÃ½ dá»¯ liá»‡u trÃ¹ng láº·p** - PhÃ¡t hiá»‡n vÃ  loáº¡i bá» duplicates  \n",
    "3. **ğŸ“ Biáº¿n Ä‘á»•i vÃ  chuáº©n hÃ³a** - ÄÆ°a dá»¯ liá»‡u vá» cÃ¹ng thang Ä‘o\n",
    "4. **ğŸ“ Xá»­ lÃ½ chuá»—i kÃ½ tá»±** - LÃ m sáº¡ch text data\n",
    "5. **ğŸ·ï¸ Xá»­ lÃ½ dá»¯ liá»‡u phÃ¢n loáº¡i** - Encoding cho ML algorithms\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ LÆ°u Ã½ quan trá»ng\n",
    "\n",
    "> **ğŸ’¡ Cho sinh viÃªn Kinh táº¿:** BÃ i há»c nÃ y táº­p trung vÃ o cÃ¡c ká»¹ thuáº­t thá»±c táº¿ mÃ  báº¡n sáº½ sá»­ dá»¥ng khi phÃ¢n tÃ­ch dá»¯ liá»‡u kinh táº¿, kháº£o sÃ¡t khÃ¡ch hÃ ng, vÃ  nghiÃªn cá»©u thá»‹ trÆ°á»ng.\n",
    "\n",
    "> **ğŸ”§ TÆ°Æ¡ng thÃ­ch:** Notebook nÃ y hoáº¡t Ä‘á»™ng tá»‘t trÃªn cáº£ **Jupyter Notebook**, **JupyterLab**, vÃ  **Google Colab**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc92628e",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Thiáº¿t láº­p mÃ´i trÆ°á»ng\n",
    "\n",
    "TrÆ°á»›c khi báº¯t Ä‘áº§u, hÃ£y Ä‘áº£m báº£o báº¡n Ä‘Ã£ cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t:\n",
    "\n",
    "**ğŸ“¦ CÃ i Ä‘áº·t thÆ° viá»‡n (cháº¡y cell nÃ y náº¿u báº¡n Ä‘ang sá»­ dá»¥ng Google Colab):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6aa4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÃ i Ä‘áº·t thÆ° viá»‡n cho Google Colab (bá» qua náº¿u Ä‘Ã£ cÃ i Ä‘áº·t)\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, LabelEncoder, OneHotEncoder, KNNImputer\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "    print(\"âœ… Táº¥t cáº£ thÆ° viá»‡n Ä‘Ã£ sáºµn sÃ ng!\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Thiáº¿u thÆ° viá»‡n: {e}\")\n",
    "    print(\"ğŸ”§ Äang cÃ i Ä‘áº·t...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    # CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t\n",
    "    packages = ['pandas', 'numpy', 'scikit-learn']\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "    \n",
    "    print(\"âœ… CÃ i Ä‘áº·t hoÃ n táº¥t! Vui lÃ²ng restart kernel vÃ  cháº¡y láº¡i cell nÃ y.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d87558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8867191",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79fbd8d0",
   "metadata": {},
   "source": [
    "# ğŸ” Pháº§n 1: Xá»­ lÃ½ dá»¯ liá»‡u thiáº¿u (Missing Data)\n",
    "\n",
    "## ğŸ“Š Táº¡i sao dá»¯ liá»‡u thiáº¿u quan trá»ng trong kinh táº¿?\n",
    "\n",
    "Trong thá»±c táº¿ kinh doanh vÃ  nghiÃªn cá»©u kinh táº¿, **dá»¯ liá»‡u thiáº¿u** lÃ  váº¥n Ä‘á» ráº¥t phá»• biáº¿n:\n",
    "\n",
    "### ğŸ¢ VÃ­ dá»¥ thá»±c táº¿ tá»« doanh nghiá»‡p:\n",
    "- **Kháº£o sÃ¡t khÃ¡ch hÃ ng**: Má»™t sá»‘ khÃ¡ch hÃ ng khÃ´ng tráº£ lá»i cÃ¢u há»i vá» thu nháº­p\n",
    "- **BÃ¡o cÃ¡o tÃ i chÃ­nh**: Má»™t sá»‘ cÃ´ng ty khÃ´ng cÃ´ng bá»‘ Ä‘áº§y Ä‘á»§ thÃ´ng tin\n",
    "- **Dá»¯ liá»‡u thá»‹ trÆ°á»ng**: GiÃ¡ cá»• phiáº¿u cÃ³ thá»ƒ bá»‹ thiáº¿u trong ngÃ y nghá»‰ lá»…\n",
    "- **NghiÃªn cá»©u kinh táº¿**: Má»™t sá»‘ há»™ gia Ä‘Ã¬nh tá»« chá»‘i cung cáº¥p thÃ´ng tin chi tiÃªu\n",
    "\n",
    "### âš ï¸ TÃ¡c Ä‘á»™ng cá»§a dá»¯ liá»‡u thiáº¿u:\n",
    "- **Giáº£m Ä‘á»™ tin cáº­y** cá»§a phÃ¢n tÃ­ch\n",
    "- **ThiÃªn lá»‡ch káº¿t quáº£** nghiÃªn cá»©u\n",
    "- **KhÃ³ khÄƒn trong dá»± bÃ¡o** kinh táº¿\n",
    "- **áº¢nh hÆ°á»Ÿng Ä‘áº¿n quyáº¿t Ä‘á»‹nh** Ä‘áº§u tÆ°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb9881",
   "metadata": {},
   "source": [
    "### Hiá»ƒu vá» dá»¯ liá»‡u thiáº¿u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d04b1",
   "metadata": {},
   "source": [
    "* Trong thá»±c táº¿, khi thu tháº­p vÃ  lÆ°u trá»¯ dá»¯ liá»‡u, khÃ´ng pháº£i lÃºc nÃ o má»i giÃ¡ trá»‹ cÅ©ng Ä‘Æ°á»£c ghi nháº­n Ä‘áº§y Ä‘á»§.\n",
    "* Má»™t sá»‘ Ã´ cÃ³ thá»ƒ bá»‹ trá»‘ng hoáº·c mang cÃ¡c kÃ½ hiá»‡u Ä‘áº·c biá»‡t nhÆ° NA, NaN, NULL, hoáº·c chuá»—i rá»—ng \"\". ÄÃ¢y chÃ­nh lÃ  dá»¯ liá»‡u thiáº¿u (missing data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a667638",
   "metadata": {},
   "source": [
    "### ğŸ” CÃ¡c dáº¡ng thiáº¿u dá»¯ liá»‡u trong kinh táº¿\n",
    "\n",
    "Trong nghiÃªn cá»©u kinh táº¿, chÃºng ta phÃ¢n loáº¡i dá»¯ liá»‡u thiáº¿u thÃ nh 3 loáº¡i chÃ­nh:\n",
    "\n",
    "#### 1ï¸âƒ£ **MCAR (Missing Completely At Random)** - Thiáº¿u hoÃ n toÃ n ngáº«u nhiÃªn\n",
    "- **VÃ­ dá»¥**: MÃ¡y tÃ­nh bá»‹ lá»—i khi thu tháº­p dá»¯ liá»‡u giÃ¡ cá»• phiáº¿u\n",
    "- **Äáº·c Ä‘iá»ƒm**: KhÃ´ng liÃªn quan Ä‘áº¿n báº¥t ká»³ yáº¿u tá»‘ nÃ o\n",
    "- **Xá»­ lÃ½**: CÃ³ thá»ƒ loáº¡i bá» an toÃ n\n",
    "\n",
    "#### 2ï¸âƒ£ **MAR (Missing At Random)** - Thiáº¿u cÃ³ Ä‘iá»u kiá»‡n\n",
    "- **VÃ­ dá»¥**: NgÆ°á»i cÃ³ thu nháº­p cao thÆ°á»ng khÃ´ng tráº£ lá»i cÃ¢u há»i vá» thu nháº­p\n",
    "- **Äáº·c Ä‘iá»ƒm**: Phá»¥ thuá»™c vÃ o cÃ¡c biáº¿n khÃ¡c cÃ³ thá»ƒ quan sÃ¡t Ä‘Æ°á»£c\n",
    "- **Xá»­ lÃ½**: Cáº§n phÃ¢n tÃ­ch cáº©n tháº­n\n",
    "\n",
    "#### 3ï¸âƒ£ **MNAR (Missing Not At Random)** - Thiáº¿u cÃ³ há»‡ thá»‘ng\n",
    "- **VÃ­ dá»¥**: CÃ´ng ty cÃ³ lá»£i nhuáº­n tháº¥p thÆ°á»ng khÃ´ng cÃ´ng bá»‘ bÃ¡o cÃ¡o tÃ i chÃ­nh\n",
    "- **Äáº·c Ä‘iá»ƒm**: LiÃªn quan trá»±c tiáº¿p Ä‘áº¿n giÃ¡ trá»‹ bá»‹ thiáº¿u\n",
    "- **Xá»­ lÃ½**: Cáº§n ká»¹ thuáº­t phá»©c táº¡p Ä‘á»ƒ xá»­ lÃ½\n",
    "\n",
    "### ğŸ“‹ Biá»ƒu diá»…n dá»¯ liá»‡u thiáº¿u trong Python:\n",
    "- `NaN` (*Not a Number*) - cho dá»¯ liá»‡u sá»‘\n",
    "- `None` - cho dá»¯ liá»‡u Ä‘á»‘i tÆ°á»£ng  \n",
    "- `NaT` (*Not a Time*) - cho dá»¯ liá»‡u thá»i gian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc26c07b",
   "metadata": {},
   "source": [
    "### ğŸ¯ NguyÃªn nhÃ¢n gÃ¢y ra dá»¯ liá»‡u thiáº¿u trong kinh táº¿\n",
    "\n",
    "#### ğŸ“Š **Lá»—i thu tháº­p dá»¯ liá»‡u**\n",
    "- **VÃ­ dá»¥**: Há»‡ thá»‘ng giao dá»‹ch chá»©ng khoÃ¡n bá»‹ sáº­p trong giá» cao Ä‘iá»ƒm\n",
    "- **TÃ¡c Ä‘á»™ng**: Máº¥t dá»¯ liá»‡u giÃ¡ cá»• phiáº¿u quan trá»ng\n",
    "\n",
    "#### ğŸ‘¥ **NgÆ°á»i dÃ¹ng khÃ´ng cung cáº¥p**\n",
    "- **VÃ­ dá»¥**: KhÃ¡ch hÃ ng bá» qua cÃ¢u há»i vá» thu nháº­p trong kháº£o sÃ¡t\n",
    "- **TÃ¡c Ä‘á»™ng**: Thiáº¿u thÃ´ng tin Ä‘á»ƒ phÃ¢n tÃ­ch hÃ nh vi tiÃªu dÃ¹ng\n",
    "\n",
    "#### ğŸ“ˆ **Dá»¯ liá»‡u khÃ´ng tá»“n táº¡i**\n",
    "- **VÃ­ dá»¥**: CÃ´ng ty má»›i thÃ nh láº­p chÆ°a cÃ³ bÃ¡o cÃ¡o tÃ i chÃ­nh nÄƒm trÆ°á»›c\n",
    "- **TÃ¡c Ä‘á»™ng**: KhÃ³ so sÃ¡nh hiá»‡u suáº¥t vá»›i cÃ¡c cÃ´ng ty khÃ¡c\n",
    "\n",
    "#### ğŸ”„ **Lá»—i xá»­ lÃ½ dá»¯ liá»‡u**\n",
    "- **VÃ­ dá»¥**: Lá»—i khi chuyá»ƒn Ä‘á»•i Ä‘á»‹nh dáº¡ng tá»« Excel sang CSV\n",
    "- **TÃ¡c Ä‘á»™ng**: Máº¥t thÃ´ng tin quan trá»ng trong quÃ¡ trÃ¬nh chuyá»ƒn Ä‘á»•i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d455ee",
   "metadata": {},
   "source": [
    "### âš ï¸ TÃ¡c Ä‘á»™ng cá»§a dá»¯ liá»‡u thiáº¿u Ä‘áº¿n phÃ¢n tÃ­ch kinh táº¿\n",
    "\n",
    "#### ğŸ“‰ **Giáº£m kÃ­ch thÆ°á»›c máº«u**\n",
    "- **VÃ­ dá»¥**: Kháº£o sÃ¡t 1000 khÃ¡ch hÃ ng, nhÆ°ng chá»‰ cÃ³ 800 ngÆ°á»i tráº£ lá»i Ä‘áº§y Ä‘á»§\n",
    "- **TÃ¡c Ä‘á»™ng**: Giáº£m Ä‘á»™ tin cáº­y cá»§a káº¿t quáº£ nghiÃªn cá»©u\n",
    "\n",
    "#### ğŸ¯ **ThiÃªn lá»‡ch káº¿t quáº£**\n",
    "- **VÃ­ dá»¥**: Chá»‰ nhá»¯ng ngÆ°á»i cÃ³ thu nháº­p cao má»›i tráº£ lá»i cÃ¢u há»i vá» thu nháº­p\n",
    "- **TÃ¡c Ä‘á»™ng**: Káº¿t quáº£ phÃ¢n tÃ­ch khÃ´ng Ä‘áº¡i diá»‡n cho toÃ n bá»™ dÃ¢n sá»‘\n",
    "\n",
    "#### ğŸ¤– **Giáº£m hiá»‡u quáº£ phÃ¢n tÃ­ch**\n",
    "- **VÃ­ dá»¥**: Thuáº­t toÃ¡n machine learning khÃ´ng thá»ƒ xá»­ lÃ½ dá»¯ liá»‡u thiáº¿u\n",
    "- **TÃ¡c Ä‘á»™ng**: KhÃ´ng thá»ƒ dá»± bÃ¡o xu hÆ°á»›ng thá»‹ trÆ°á»ng chÃ­nh xÃ¡c\n",
    "\n",
    "#### ğŸ’¼ **áº¢nh hÆ°á»Ÿng quyáº¿t Ä‘á»‹nh kinh doanh**\n",
    "- **VÃ­ dá»¥**: Thiáº¿u dá»¯ liá»‡u vá» Ä‘á»‘i thá»§ cáº¡nh tranh\n",
    "- **TÃ¡c Ä‘á»™ng**: Ra quyáº¿t Ä‘á»‹nh Ä‘áº§u tÆ° khÃ´ng chÃ­nh xÃ¡c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e720916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š VÃ­ dá»¥ thá»±c táº¿: Dá»¯ liá»‡u nhÃ¢n viÃªn cÃ´ng ty cÃ³ missing values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Táº¡o DataFrame máº«u vá» nhÃ¢n viÃªn cÃ´ng ty (tÃ¬nh huá»‘ng thá»±c táº¿)\n",
    "print(\"ğŸ¢ VÃ Dá»¤: Dá»¯ liá»‡u nhÃ¢n viÃªn cÃ´ng ty cÃ³ missing values\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data_nhanvien = {\n",
    "    'TenNV': ['Nguyá»…n VÄƒn A', 'Tráº§n Thá»‹ B', 'LÃª VÄƒn C', 'Pháº¡m Thá»‹ D', 'HoÃ ng VÄƒn E'],\n",
    "    'Tuoi': [25, None, 30, 28, None],  # Má»™t sá»‘ nhÃ¢n viÃªn khÃ´ng cung cáº¥p tuá»•i\n",
    "    'Luong': [15000000, 18000000, None, 22000000, 16000000],  # LÆ°Æ¡ng bá»‹ thiáº¿u\n",
    "    'PhongBan': ['IT', 'Marketing', None, 'IT', 'Marketing'],  # PhÃ²ng ban khÃ´ng rÃµ\n",
    "    'KinhNghiem': [2, 5, None, 7, 1]  # Kinh nghiá»‡m chÆ°a Ä‘Æ°á»£c cáº­p nháº­t\n",
    "}\n",
    "\n",
    "df_nhanvien = pd.DataFrame(data_nhanvien)\n",
    "print(\"ğŸ“‹ DataFrame nhÃ¢n viÃªn vá»›i dá»¯ liá»‡u thiáº¿u:\")\n",
    "print(df_nhanvien)\n",
    "\n",
    "print(f\"\\nğŸ“Š ThÃ´ng tin tá»•ng quan:\")\n",
    "print(f\"   - Tá»•ng sá»‘ nhÃ¢n viÃªn: {len(df_nhanvien)}\")\n",
    "print(f\"   - Sá»‘ cá»™t: {len(df_nhanvien.columns)}\")\n",
    "print(f\"   - Kiá»ƒu dá»¯ liá»‡u:\")\n",
    "print(df_nhanvien.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c84ff4",
   "metadata": {},
   "source": [
    "### ğŸ” CÃ¡c phÆ°Æ¡ng thá»©c phÃ¡t hiá»‡n dá»¯ liá»‡u thiáº¿u trong pandas\n",
    "\n",
    "Pandas cung cáº¥p cÃ¡c phÆ°Æ¡ng thá»©c chuyÃªn dá»¥ng Ä‘á»ƒ **phÃ¡t hiá»‡n vÃ  kiá»ƒm tra dá»¯ liá»‡u thiáº¿u**:\n",
    "\n",
    "| PhÆ°Æ¡ng thá»©c | MÃ´ táº£ | Tráº£ vá» | VÃ­ dá»¥ sá»­ dá»¥ng |\n",
    "|-------------|-------|--------|---------------|\n",
    "| `isna()` / `isnull()` | Kiá»ƒm tra tá»«ng pháº§n tá»­ cÃ³ thiáº¿u khÃ´ng | Boolean DataFrame/Series | `df.isna()` |\n",
    "| `notna()` / `notnull()` | Kiá»ƒm tra tá»«ng pháº§n tá»­ cÃ³ dá»¯ liá»‡u khÃ´ng | Boolean DataFrame/Series | `df.notna()` |\n",
    "| `isna().sum()` | Äáº¿m sá»‘ lÆ°á»£ng dá»¯ liá»‡u thiáº¿u theo cá»™t | Series vá»›i sá»‘ lÆ°á»£ng | `df.isna().sum()` |\n",
    "| `isna().any()` | Kiá»ƒm tra cÃ³ cá»™t nÃ o thiáº¿u dá»¯ liá»‡u khÃ´ng | Boolean Series | `df.isna().any()` |\n",
    "\n",
    "**ğŸ“Š HÃ£y xem cÃ¡ch sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng thá»©c nÃ y vá»›i dá»¯ liá»‡u nhÃ¢n viÃªn:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ba1fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda1550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” DEMO: PhÃ¡t hiá»‡n dá»¯ liá»‡u thiáº¿u trong DataFrame nhÃ¢n viÃªn\n",
    "print(\"ğŸ” DEMO: CÃ¡c phÆ°Æ¡ng thá»©c phÃ¡t hiá»‡n dá»¯ liá»‡u thiáº¿u\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sá»­ dá»¥ng DataFrame tá»« cell trÆ°á»›c\n",
    "print(\"ğŸ“‹ DataFrame gá»‘c:\")\n",
    "print(df_nhanvien)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1ï¸âƒ£ KIá»‚M TRA Tá»ªNG PHáº¦N Tá»¬ CÃ“ THIáº¾U KHÃ”NG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Kiá»ƒm tra dá»¯ liá»‡u thiáº¿u - tráº£ vá» Boolean DataFrame\n",
    "print(\"ğŸ” df_nhanvien.isna() - Kiá»ƒm tra tá»«ng Ã´ cÃ³ thiáº¿u khÃ´ng:\")\n",
    "print(df_nhanvien.isna())\n",
    "\n",
    "print(\"\\nğŸ” df_nhanvien.notna() - Kiá»ƒm tra tá»«ng Ã´ cÃ³ dá»¯ liá»‡u khÃ´ng:\")\n",
    "print(df_nhanvien.notna())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2ï¸âƒ£ Äáº¾M Sá» LÆ¯á»¢NG Dá»® LIá»†U THIáº¾U THEO Cá»˜T\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 2. Äáº¿m sá»‘ lÆ°á»£ng dá»¯ liá»‡u thiáº¿u theo tá»«ng cá»™t\n",
    "print(\"ğŸ“Š df_nhanvien.isna().sum() - Sá»‘ lÆ°á»£ng missing values theo cá»™t:\")\n",
    "missing_count = df_nhanvien.isna().sum()\n",
    "print(missing_count)\n",
    "\n",
    "# TÃ­nh pháº§n trÄƒm missing\n",
    "print(\"\\nğŸ“ˆ Tá»· lá»‡ missing values (%):\")\n",
    "missing_percent = (missing_count / len(df_nhanvien)) * 100\n",
    "print(missing_percent.round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3ï¸âƒ£ KIá»‚M TRA Cá»˜T NÃ€O CÃ“ Dá»® LIá»†U THIáº¾U\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 3. Kiá»ƒm tra cá»™t nÃ o cÃ³ dá»¯ liá»‡u thiáº¿u\n",
    "print(\"ğŸ” df_nhanvien.isna().any() - Cá»™t nÃ o cÃ³ missing values:\")\n",
    "print(df_nhanvien.isna().any())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4ï¸âƒ£ Tá»”NG Sá» Dá»® LIá»†U THIáº¾U\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 4. Tá»•ng sá»‘ dá»¯ liá»‡u thiáº¿u trong toÃ n bá»™ DataFrame\n",
    "total_missing = df_nhanvien.isna().sum().sum()\n",
    "total_cells = df_nhanvien.shape[0] * df_nhanvien.shape[1]\n",
    "missing_percentage = (total_missing / total_cells) * 100\n",
    "\n",
    "print(f\"ğŸ“Š Tá»•ng sá»‘ missing values: {total_missing}\")\n",
    "print(f\"ğŸ“Š Tá»•ng sá»‘ Ã´ dá»¯ liá»‡u: {total_cells}\")\n",
    "print(f\"ğŸ“Š Tá»· lá»‡ missing: {missing_percentage:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b23b439",
   "metadata": {},
   "source": [
    "### ğŸ› ï¸ CÃ¡c phÆ°Æ¡ng phÃ¡p xá»­ lÃ½ dá»¯ liá»‡u thiáº¿u trong kinh táº¿\n",
    "\n",
    "**ğŸ¯ CÃ³ 3 cÃ¡ch chÃ­nh Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u thiáº¿u:**\n",
    "\n",
    "#### 1ï¸âƒ£ **Loáº¡i bá»** (*Deletion*) \n",
    "- **Khi nÃ o dÃ¹ng**: Dá»¯ liá»‡u thiáº¿u < 5%, thiáº¿u ngáº«u nhiÃªn\n",
    "- **VÃ­ dá»¥**: Loáº¡i bá» khÃ¡ch hÃ ng khÃ´ng tráº£ lá»i Ä‘áº§y Ä‘á»§ kháº£o sÃ¡t\n",
    "- **Æ¯u Ä‘iá»ƒm**: ÄÆ¡n giáº£n, khÃ´ng táº¡o bias\n",
    "- **NhÆ°á»£c Ä‘iá»ƒm**: Giáº£m kÃ­ch thÆ°á»›c máº«u\n",
    "\n",
    "#### 2ï¸âƒ£ **Thay tháº¿** (*Imputation*)\n",
    "- **Khi nÃ o dÃ¹ng**: Dá»¯ liá»‡u thiáº¿u 5-20%, cÃ³ pattern\n",
    "- **VÃ­ dá»¥**: Thay tháº¿ lÆ°Æ¡ng thiáº¿u báº±ng lÆ°Æ¡ng trung bÃ¬nh cá»§a phÃ²ng ban\n",
    "- **Æ¯u Ä‘iá»ƒm**: Giá»¯ nguyÃªn kÃ­ch thÆ°á»›c máº«u\n",
    "- **NhÆ°á»£c Ä‘iá»ƒm**: CÃ³ thá»ƒ táº¡o bias\n",
    "\n",
    "#### 3ï¸âƒ£ **Dá»± Ä‘oÃ¡n** (*Prediction*)\n",
    "- **Khi nÃ o dÃ¹ng**: Dá»¯ liá»‡u thiáº¿u > 20%, cÃ³ má»‘i quan há»‡ phá»©c táº¡p\n",
    "- **VÃ­ dá»¥**: DÃ¹ng machine learning Ä‘á»ƒ dá»± Ä‘oÃ¡n thu nháº­p dá»±a trÃªn cÃ¡c yáº¿u tá»‘ khÃ¡c\n",
    "- **Æ¯u Ä‘iá»ƒm**: ChÃ­nh xÃ¡c cao\n",
    "- **NhÆ°á»£c Ä‘iá»ƒm**: Phá»©c táº¡p, cáº§n hiá»ƒu biáº¿t vá» ML\n",
    "\n",
    "**âš–ï¸ HÆ°á»›ng dáº«n lá»±a chá»n phÆ°Æ¡ng phÃ¡p:**\n",
    "\n",
    "| Tá»· lá»‡ thiáº¿u | Loáº¡i dá»¯ liá»‡u | PhÆ°Æ¡ng phÃ¡p khuyáº¿n nghá»‹ |\n",
    "|-------------|--------------|------------------------|\n",
    "| < 5% | Báº¥t ká»³ | Loáº¡i bá» |\n",
    "| 5-20% | Sá»‘ | Mean/Median |\n",
    "| 5-20% | PhÃ¢n loáº¡i | Mode |\n",
    "| > 20% | CÃ³ quan há»‡ | Machine Learning |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27c281",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e8cddba",
   "metadata": {},
   "source": [
    "#### **PhÆ°Æ¡ng phÃ¡p 1: Loáº¡i bá» dá»¯ liá»‡u thiáº¿u (`dropna`)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b627f4",
   "metadata": {},
   "source": [
    "PhÆ°Æ¡ng thá»©c `dropna()` cho phÃ©p loáº¡i bá» cÃ¡c hÃ ng hoáº·c cá»™t cÃ³ dá»¯ liá»‡u thiáº¿u:\n",
    "\n",
    "**ğŸ“‹ CÃ¡c tham sá»‘ quan trá»ng cá»§a `dropna()`:**\n",
    "\n",
    "| Tham sá»‘ | GiÃ¡ trá»‹ | MÃ´ táº£ |\n",
    "|---------|---------|-------|\n",
    "| `axis` | 0 (default) / 1 | 0: loáº¡i bá» hÃ ng, 1: loáº¡i bá» cá»™t |\n",
    "| `how` | 'any' (default) / 'all' | 'any': cÃ³ Ã­t nháº¥t 1 NaN, 'all': toÃ n bá»™ lÃ  NaN |\n",
    "| `subset` | list | Chá»‰ xÃ©t dá»¯ liá»‡u thiáº¿u trong cÃ¡c cá»™t Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh |\n",
    "| `thresh` | int | Sá»‘ lÆ°á»£ng giÃ¡ trá»‹ khÃ´ng null tá»‘i thiá»ƒu cáº§n cÃ³ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Táº¡o DataFrame máº«u vá»›i dá»¯ liá»‡u thiáº¿u\n",
    "data_missing = {\n",
    "    'TÃªn': ['An', 'BÃ¬nh', 'Chi', 'DÅ©ng', 'Eva'],\n",
    "    'Tuá»•i': [25, None, 30, 28, None],\n",
    "    'LÆ°Æ¡ng': [15000000, 18000000, None, 22000000, 16000000],\n",
    "    'PhÃ²ng ban': ['IT', 'Marketing', None, 'IT', 'Marketing']\n",
    "}\n",
    "\n",
    "df_missing = pd.DataFrame(data_missing)\n",
    "print(\"DataFrame vá»›i dá»¯ liá»‡u thiáº¿u:\")\n",
    "print(df_missing)\n",
    "\n",
    "# 1. Loáº¡i bá» táº¥t cáº£ hÃ ng cÃ³ Ã­t nháº¥t 1 giÃ¡ trá»‹ thiáº¿u\n",
    "print(\"1. Loáº¡i bá» hÃ ng cÃ³ dá»¯ liá»‡u thiáº¿u (how='any'):\")\n",
    "df_drop_any = df_missing.dropna()\n",
    "print(df_drop_any)\n",
    "print(f\"Sá»‘ hÃ ng cÃ²n láº¡i: {len(df_drop_any)}\")\n",
    "\n",
    "# 2. Loáº¡i bá» hÃ ng chá»‰ khi Táº¤T Cáº¢ giÃ¡ trá»‹ Ä‘á»u thiáº¿u\n",
    "print(\"2. Loáº¡i bá» hÃ ng khi táº¥t cáº£ giÃ¡ trá»‹ Ä‘á»u thiáº¿u (how='all'):\")\n",
    "df_drop_all = df_missing.dropna(how='all')\n",
    "print(df_drop_all)\n",
    "print(f\"Sá»‘ hÃ ng cÃ²n láº¡i: {len(df_drop_all)}\")\n",
    "\n",
    "# 3. Loáº¡i bá» cá»™t cÃ³ dá»¯ liá»‡u thiáº¿u\n",
    "print(\"3. Loáº¡i bá» cá»™t cÃ³ dá»¯ liá»‡u thiáº¿u (axis=1):\")\n",
    "df_drop_cols = df_missing.dropna(axis=1)\n",
    "print(df_drop_cols)\n",
    "print(f\"Sá»‘ cá»™t cÃ²n láº¡i: {len(df_drop_cols.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee315f8",
   "metadata": {},
   "source": [
    "#### **PhÆ°Æ¡ng phÃ¡p 2: Thay tháº¿ dá»¯ liá»‡u thiáº¿u (`fillna`)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff3235",
   "metadata": {},
   "source": [
    "PhÆ°Æ¡ng thá»©c `fillna()` cho phÃ©p **thay tháº¿ dá»¯ liá»‡u thiáº¿u** báº±ng cÃ¡c giÃ¡ trá»‹ cá»¥ thá»ƒ:\n",
    "\n",
    "**ğŸ”§ CÃ¡c chiáº¿n lÆ°á»£c thay tháº¿ phá»• biáº¿n:**\n",
    "\n",
    "| Chiáº¿n lÆ°á»£c | á»¨ng dá»¥ng | VÃ­ dá»¥ |\n",
    "|------------|----------|-------|\n",
    "| **GiÃ¡ trá»‹ cá»‘ Ä‘á»‹nh** | Thay tháº¿ báº±ng má»™t giÃ¡ trá»‹ nháº¥t Ä‘á»‹nh | `fillna(0)`, `fillna('Unknown')` |\n",
    "| **GiÃ¡ trá»‹ trung bÃ¬nh** | Dá»¯ liá»‡u sá»‘, phÃ¢n phá»‘i chuáº©n | `fillna(df['col'].mean())` |\n",
    "| **GiÃ¡ trá»‹ trung vá»‹** | Dá»¯ liá»‡u sá»‘, cÃ³ outliers | `fillna(df['col'].median())` |\n",
    "| **GiÃ¡ trá»‹ phá»• biáº¿n nháº¥t** | Dá»¯ liá»‡u phÃ¢n loáº¡i | `fillna(df['col'].mode()[0])` |\n",
    "| **Forward fill** | Dá»¯ liá»‡u chuá»—i thá»i gian | `fillna(method='ffill')` |\n",
    "| **Backward fill** | Dá»¯ liá»‡u chuá»—i thá»i gian | `fillna(method='bfill')` |\n",
    "\n",
    "**ğŸ“Š HÃ£y xem cÃ¡c vÃ­ dá»¥ cá»¥ thá»ƒ:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d63477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Táº¡o DataFrame máº«u vá»›i dá»¯ liá»‡u thiáº¿u\n",
    "data_missing = {\n",
    "    'TÃªn': ['An', 'BÃ¬nh', 'Chi', 'DÅ©ng', 'Eva'],\n",
    "    'Tuá»•i': [25, None, 30, 28, None],\n",
    "    'LÆ°Æ¡ng': [15000000, 18000000, None, 22000000, 16000000],\n",
    "    'PhÃ²ng ban': ['IT', 'Marketing', None, 'IT', 'Marketing']\n",
    "}\n",
    "\n",
    "df_missing = pd.DataFrame(data_missing)\n",
    "print(\"DataFrame vá»›i dá»¯ liá»‡u thiáº¿u:\")\n",
    "print(df_missing)\n",
    "\n",
    "# 1. Thay tháº¿ báº±ng giÃ¡ trá»‹ cá»‘ Ä‘á»‹nh\n",
    "print(\"\\n1. Thay tháº¿ báº±ng giÃ¡ trá»‹ cá»‘ Ä‘á»‹nh:\")\n",
    "df_fill_fixed = df_missing.fillna({'Tuá»•i': 0, 'LÆ°Æ¡ng': 0, 'PhÃ²ng ban': 'ChÆ°a xÃ¡c Ä‘á»‹nh'})\n",
    "print(df_fill_fixed)\n",
    "\n",
    "# 2. Thay tháº¿ báº±ng giÃ¡ trá»‹ trung bÃ¬nh (cho dá»¯ liá»‡u sá»‘)\n",
    "print(\"\\n2. Thay tháº¿ báº±ng giÃ¡ trá»‹ trung bÃ¬nh:\")\n",
    "df_fill_mean = df_missing.copy()\n",
    "df_fill_mean['Tuá»•i'] = df_fill_mean['Tuá»•i'].fillna(df_fill_mean['Tuá»•i'].mean())\n",
    "df_fill_mean['LÆ°Æ¡ng'] = df_fill_mean['LÆ°Æ¡ng'].fillna(df_fill_mean['LÆ°Æ¡ng'].mean())\n",
    "print(df_fill_mean)\n",
    "print(f\"Tuá»•i trung bÃ¬nh: {df_missing['Tuá»•i'].mean():.1f}\")\n",
    "print(f\"LÆ°Æ¡ng trung bÃ¬nh: {df_missing['LÆ°Æ¡ng'].mean():,.0f}\")\n",
    "\n",
    "# 3. Thay tháº¿ báº±ng giÃ¡ trá»‹ trung vá»‹ (bá»n vá»¯ng vá»›i outliers)\n",
    "print(\"\\n3. Thay tháº¿ báº±ng giÃ¡ trá»‹ trung vá»‹:\")\n",
    "df_fill_median = df_missing.copy()\n",
    "df_fill_median['Tuá»•i'] = df_fill_median['Tuá»•i'].fillna(df_fill_median['Tuá»•i'].median())\n",
    "df_fill_median['LÆ°Æ¡ng'] = df_fill_median['LÆ°Æ¡ng'].fillna(df_fill_median['LÆ°Æ¡ng'].median())\n",
    "print(df_fill_median)\n",
    "\n",
    "# 4. Thay tháº¿ báº±ng giÃ¡ trá»‹ phá»• biáº¿n nháº¥t (mode) - cho dá»¯ liá»‡u phÃ¢n loáº¡i\n",
    "print(\"\\n4. Thay tháº¿ báº±ng giÃ¡ trá»‹ phá»• biáº¿n nháº¥t (mode):\")\n",
    "df_fill_mode = df_missing.copy()\n",
    "df_fill_mode['PhÃ²ng ban'] = df_fill_mode['PhÃ²ng ban'].fillna(df_fill_mode['PhÃ²ng ban'].mode()[0])\n",
    "print(df_fill_mode)\n",
    "print(f\"PhÃ²ng ban phá»• biáº¿n nháº¥t: {df_missing['PhÃ²ng ban'].mode()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff8a4a",
   "metadata": {},
   "source": [
    "#### PhÆ°Æ¡ng phÃ¡p 3: Sá»­ dá»¥ng mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68faa286",
   "metadata": {},
   "source": [
    "Sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh **dá»± Ä‘oÃ¡n** Ä‘á»ƒ Æ°á»›c lÆ°á»£ng giÃ¡ trá»‹ thiáº¿u.\n",
    "\n",
    "**ğŸ”§ CÃ¡c chiáº¿n lÆ°á»£c thay tháº¿ phá»• biáº¿n:**\n",
    "\n",
    "| Chiáº¿n lÆ°á»£c | á»¨ng dá»¥ng | VÃ­ dá»¥ |\n",
    "|------------|----------|-------|\n",
    "| **Há»“i quy** | Dá»¯ liá»‡u sá»‘ | Sá»­ dá»¥ng há»“i quy tuyáº¿n tÃ­nh Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ |\n",
    "| **PhÃ¢n loáº¡i** | Dá»¯ liá»‡u phÃ¢n loáº¡i | Sá»­ dá»¥ng cÃ¢y quyáº¿t Ä‘á»‹nh Ä‘á»ƒ phÃ¢n loáº¡i giÃ¡ trá»‹ |\n",
    "| **PhÃ¢n tÃ­ch thá»‘ng kÃª** | Dá»¯ liá»‡u sá»‘ | Sá»­ dá»¥ng thá»‘ng kÃª Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ |\n",
    "\n",
    "**ğŸ“Š HÃ£y xem cÃ¡c vÃ­ dá»¥ cá»¥ thá»ƒ:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b53ebe",
   "metadata": {},
   "source": [
    "**ğŸ¤– Khi nÃ o sá»­ dá»¥ng Machine Learning cho Missing Values:**\n",
    "\n",
    "- **Dá»¯ liá»‡u cÃ³ má»‘i quan há»‡ phá»©c táº¡p**: CÃ¡c biáº¿n cÃ³ correlation cao vá»›i nhau\n",
    "- **Dá»¯ liá»‡u missing khÃ´ng ngáº«u nhiÃªn**: Missing data cÃ³ pattern Ä‘áº·c biá»‡t\n",
    "- **Dá»¯ liá»‡u quan trá»ng**: KhÃ´ng muá»‘n máº¥t thÃ´ng tin báº±ng cÃ¡ch loáº¡i bá»\n",
    "- **YÃªu cáº§u Ä‘á»™ chÃ­nh xÃ¡c cao**: Muá»‘n dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c nháº¥t cÃ³ thá»ƒ\n",
    "\n",
    "**âš¡ CÃ¡c ká»¹ thuáº­t Machine Learning phá»• biáº¿n:**\n",
    "\n",
    "- **Regression**: Dá»± Ä‘oÃ¡n giÃ¡ trá»‹ sá»‘ (Linear, Random Forest, XGBoost)\n",
    "- **Classification**: Dá»± Ä‘oÃ¡n giÃ¡ trá»‹ phÃ¢n loáº¡i (Decision Tree, SVM)\n",
    "- **Clustering**: NhÃ³m cÃ¡c quan sÃ¡t tÆ°Æ¡ng tá»± (K-Means, DBSCAN)\n",
    "- **Deep Learning**: Neural Networks cho dá»¯ liá»‡u phá»©c táº¡p\n",
    "\n",
    "**ğŸ¯ Æ¯u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm:**\n",
    "\n",
    "âœ… **Æ¯u Ä‘iá»ƒm:**\n",
    "- Äá»™ chÃ­nh xÃ¡c cao hÆ¡n mean/median/mode\n",
    "- Táº­n dá»¥ng Ä‘Æ°á»£c má»‘i quan há»‡ giá»¯a cÃ¡c biáº¿n\n",
    "- Linh hoáº¡t vá»›i nhiá»u loáº¡i dá»¯ liá»‡u\n",
    "\n",
    "âŒ **NhÆ°á»£c Ä‘iá»ƒm:**\n",
    "- Phá»©c táº¡p, cáº§n hiá»ƒu biáº¿t vá» ML\n",
    "- Tá»‘n thá»i gian training\n",
    "- CÃ³ thá»ƒ overfitting náº¿u khÃ´ng cáº©n tháº­n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf2cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t cho machine learning\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Táº¡o DataFrame máº«u vá»›i dá»¯ liá»‡u thiáº¿u phá»©c táº¡p hÆ¡n\n",
    "print(\"ğŸ“Š Táº¡o dá»¯ liá»‡u máº«u cho cÃ¡c vÃ­ dá»¥ Machine Learning:\")\n",
    "\n",
    "data_advanced = {\n",
    "    'TÃªn': ['An', 'BÃ¬nh', 'Chi', 'DÅ©ng', 'Eva', 'Phong', 'Giang', 'Hoa'],\n",
    "    'Tuá»•i': [25, None, 30, 28, None, 35, None, 32],\n",
    "    'LÆ°Æ¡ng': [15000000, 18000000, None, 22000000, 16000000, None, 25000000, None],\n",
    "    'PhÃ²ng ban': ['IT', 'Marketing', None, 'IT', 'Marketing', 'IT', 'HR', 'HR'],\n",
    "    'Kinh nghiá»‡m': [2, 5, 7, 3, 1, 10, 8, 6]\n",
    "}\n",
    "\n",
    "df_advanced = pd.DataFrame(data_advanced)\n",
    "print(\"Dá»¯ liá»‡u gá»‘c:\")\n",
    "print(df_advanced)\n",
    "print(f\"\\nğŸ“ˆ Tá»· lá»‡ missing data:\")\n",
    "missing_percent = (df_advanced.isnull().sum() / len(df_advanced)) * 100\n",
    "print(missing_percent[missing_percent > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd93f66",
   "metadata": {},
   "source": [
    "#### **A. Random Forest Regression - Dá»± Ä‘oÃ¡n giÃ¡ trá»‹ sá»‘**\n",
    "\n",
    "**ğŸŒ² Random Forest lÃ  gÃ¬?**\n",
    "\n",
    "Random Forest lÃ  thuáº­t toÃ¡n **ensemble learning** káº¿t há»£p nhiá»u **Decision Trees**:\n",
    "\n",
    "- **Ensemble**: Káº¿t há»£p nhiá»u mÃ´ hÃ¬nh yáº¿u thÃ nh má»™t mÃ´ hÃ¬nh máº¡nh\n",
    "- **Bootstrap Aggregating**: Má»—i tree Ä‘Æ°á»£c train trÃªn má»™t subset ngáº«u nhiÃªn cá»§a dá»¯ liá»‡u\n",
    "- **Feature Randomness**: Má»—i split chá»‰ xÃ©t má»™t subset ngáº«u nhiÃªn cá»§a features\n",
    "- **Voting**: Káº¿t quáº£ cuá»‘i cÃ¹ng lÃ  trung bÃ¬nh cá»§a táº¥t cáº£ trees (regression) hoáº·c vote Ä‘a sá»‘ (classification)\n",
    "\n",
    "**ğŸ¯ Æ¯u Ä‘iá»ƒm cá»§a Random Forest:**\n",
    "- **Robust**: Ãt bá»‹ overfitting nhá» averaging nhiá»u trees\n",
    "- **Handle Missing Values**: CÃ³ thá»ƒ xá»­ lÃ½ missing values trong quÃ¡ trÃ¬nh training\n",
    "- **Feature Importance**: Cung cáº¥p thÃ´ng tin vá» táº§m quan trá»ng cá»§a tá»«ng feature\n",
    "- **Non-linear**: CÃ³ thá»ƒ há»c Ä‘Æ°á»£c cÃ¡c má»‘i quan há»‡ phi tuyáº¿n phá»©c táº¡p\n",
    "\n",
    "**âš™ï¸ CÃ¡c tham sá»‘ quan trá»ng:**\n",
    "- `n_estimators`: Sá»‘ lÆ°á»£ng trees (default=100)\n",
    "- `max_depth`: Äá»™ sÃ¢u tá»‘i Ä‘a cá»§a tree\n",
    "- `min_samples_split`: Sá»‘ sample tá»‘i thiá»ƒu Ä‘á»ƒ split node\n",
    "- `random_state`: Seed cho reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf59cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ DEMO 1: Dá»± Ä‘oÃ¡n Tuá»•i dá»±a trÃªn LÆ°Æ¡ng, PhÃ²ng ban vÃ  Kinh nghiá»‡m\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u\n",
    "df_predict_age = df_advanced.copy()\n",
    "\n",
    "# Encode categorical data (PhÃ²ng ban)\n",
    "le_dept = LabelEncoder()\n",
    "df_predict_age['PhÃ²ng ban_encoded'] = le_dept.fit_transform(df_predict_age['PhÃ²ng ban'].fillna('Unknown'))\n",
    "\n",
    "print(\"ğŸ“‹ Mapping PhÃ²ng ban:\")\n",
    "dept_mapping = dict(zip(le_dept.classes_, le_dept.transform(le_dept.classes_)))\n",
    "print(dept_mapping)\n",
    "\n",
    "# BÆ°á»›c 2: TÃ¡ch dá»¯ liá»‡u train vÃ  missing\n",
    "train_data = df_predict_age[df_predict_age['Tuá»•i'].notna()]\n",
    "missing_data = df_predict_age[df_predict_age['Tuá»•i'].isna()]\n",
    "\n",
    "print(f\"\\nğŸ“Š Dá»¯ liá»‡u train: {len(train_data)} samples\")\n",
    "print(f\"ğŸ“Š Dá»¯ liá»‡u cáº§n dá»± Ä‘oÃ¡n: {len(missing_data)} samples\")\n",
    "\n",
    "if len(train_data) > 0 and len(missing_data) > 0:\n",
    "    # BÆ°á»›c 3: Chuáº©n bá»‹ features vÃ  target\n",
    "    features = ['LÆ°Æ¡ng', 'PhÃ²ng ban_encoded', 'Kinh nghiá»‡m']\n",
    "    X_train = train_data[features].fillna(0)  # Fillna táº¡m thá»i cho missing features\n",
    "    y_train = train_data['Tuá»•i']\n",
    "    \n",
    "    print(f\"\\nğŸ”§ Features sá»­ dá»¥ng: {features}\")\n",
    "    print(\"ğŸ“ˆ Training data:\")\n",
    "    print(X_train)\n",
    "    print(f\"\\nğŸ¯ Target (Tuá»•i):\")\n",
    "    print(y_train.values)\n",
    "    \n",
    "    # BÆ°á»›c 4: Training model\n",
    "    model_age = RandomForestRegressor(n_estimators=10, random_state=42, max_depth=3)\n",
    "    model_age.fit(X_train, y_train)\n",
    "    \n",
    "    # BÆ°á»›c 5: Dá»± Ä‘oÃ¡n\n",
    "    X_missing = missing_data[features].fillna(0)\n",
    "    predicted_ages = model_age.predict(X_missing)\n",
    "    \n",
    "    print(f\"\\nğŸ”® Dá»± Ä‘oÃ¡n cho {len(missing_data)} samples:\")\n",
    "    for i, (idx, row) in enumerate(missing_data.iterrows()):\n",
    "        print(f\"   {row['TÃªn']}: {predicted_ages[i]:.1f} tuá»•i\")\n",
    "    \n",
    "    # BÆ°á»›c 6: Cáº­p nháº­t dá»¯ liá»‡u\n",
    "    df_predict_age.loc[df_predict_age['Tuá»•i'].isna(), 'Tuá»•i'] = predicted_ages\n",
    "    \n",
    "    print(f\"\\nâœ… Káº¿t quáº£ cuá»‘i cÃ¹ng:\")\n",
    "    print(df_predict_age[['TÃªn', 'Tuá»•i', 'LÆ°Æ¡ng', 'PhÃ²ng ban', 'Kinh nghiá»‡m']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abcf633",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ¯ DEMO 2: Dá»± Ä‘oÃ¡n LÆ°Æ¡ng dá»±a trÃªn Tuá»•i, PhÃ²ng ban vÃ  Kinh nghiá»‡m\") \n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sá»­ dá»¥ng dá»¯ liá»‡u gá»‘c (chÆ°a cÃ³ Tuá»•i Ä‘Æ°á»£c dá»± Ä‘oÃ¡n)\n",
    "df_predict_salary = df_advanced.copy()\n",
    "df_predict_salary['PhÃ²ng ban_encoded'] = le_dept.fit_transform(df_predict_salary['PhÃ²ng ban'].fillna('Unknown'))\n",
    "\n",
    "# TÃ¡ch dá»¯ liá»‡u train vÃ  missing cho LÆ°Æ¡ng\n",
    "train_salary = df_predict_salary[df_predict_salary['LÆ°Æ¡ng'].notna()]\n",
    "missing_salary = df_predict_salary[df_predict_salary['LÆ°Æ¡ng'].isna()]\n",
    "\n",
    "print(f\"ğŸ“Š Dá»¯ liá»‡u train: {len(train_salary)} samples\")\n",
    "print(f\"ğŸ“Š Dá»¯ liá»‡u cáº§n dá»± Ä‘oÃ¡n: {len(missing_salary)} samples\")\n",
    "\n",
    "if len(train_salary) > 0 and len(missing_salary) > 0:\n",
    "    # Features cho dá»± Ä‘oÃ¡n lÆ°Æ¡ng\n",
    "    salary_features = ['Tuá»•i', 'PhÃ²ng ban_encoded', 'Kinh nghiá»‡m'] \n",
    "    X_train_salary = train_salary[salary_features].fillna(0)\n",
    "    y_train_salary = train_salary['LÆ°Æ¡ng']\n",
    "    \n",
    "    print(f\"\\nğŸ”§ Features sá»­ dá»¥ng: {salary_features}\")\n",
    "    print(\"ğŸ“ˆ Training data:\")\n",
    "    combined_train = pd.concat([X_train_salary, y_train_salary], axis=1)\n",
    "    print(combined_train)\n",
    "    \n",
    "    # Training model cho lÆ°Æ¡ng\n",
    "    model_salary = RandomForestRegressor(n_estimators=10, random_state=42, max_depth=3)\n",
    "    model_salary.fit(X_train_salary, y_train_salary)\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = model_salary.feature_importances_\n",
    "    print(f\"\\nğŸ“Š Feature Importance:\")\n",
    "    for feature, imp in zip(salary_features, importance):\n",
    "        print(f\"   {feature}: {imp:.3f}\")\n",
    "    \n",
    "    # Dá»± Ä‘oÃ¡n lÆ°Æ¡ng\n",
    "    X_missing_salary = missing_salary[salary_features].fillna(0)\n",
    "    predicted_salaries = model_salary.predict(X_missing_salary)\n",
    "    \n",
    "    print(f\"\\nğŸ”® Dá»± Ä‘oÃ¡n lÆ°Æ¡ng:\")\n",
    "    for i, (idx, row) in enumerate(missing_salary.iterrows()):\n",
    "        print(f\"   {row['TÃªn']}: {predicted_salaries[i]:,.0f} VND\")\n",
    "    \n",
    "    # Cáº­p nháº­t dá»¯ liá»‡u\n",
    "    df_predict_salary.loc[df_predict_salary['LÆ°Æ¡ng'].isna(), 'LÆ°Æ¡ng'] = predicted_salaries\n",
    "    \n",
    "    print(f\"\\nâœ… Káº¿t quáº£ cuá»‘i cÃ¹ng:\")\n",
    "    print(df_predict_salary[['TÃªn', 'Tuá»•i', 'LÆ°Æ¡ng', 'PhÃ²ng ban', 'Kinh nghiá»‡m']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b745f",
   "metadata": {},
   "source": [
    "#### **B. KNN Imputation - K-Nearest Neighbors**\n",
    "\n",
    "**ğŸ” KNN Imputation lÃ  gÃ¬?**\n",
    "\n",
    "KNN Imputation sá»­ dá»¥ng thuáº­t toÃ¡n **K-Nearest Neighbors** Ä‘á»ƒ Ä‘iá»n missing values:\n",
    "\n",
    "1. **TÃ¬m K neighbors gáº§n nháº¥t**: Dá»±a trÃªn khoáº£ng cÃ¡ch Euclidean trong khÃ´ng gian features\n",
    "2. **TÃ­nh giÃ¡ trá»‹ trung bÃ¬nh**: Láº¥y trung bÃ¬nh cá»§a K neighbors (cho sá»‘) hoáº·c mode (cho categorical)\n",
    "3. **Äiá»n vÃ o missing values**: Thay tháº¿ missing value báº±ng giÃ¡ trá»‹ Ä‘Æ°á»£c tÃ­nh\n",
    "\n",
    "**ğŸ“ CÃ´ng thá»©c khoáº£ng cÃ¡ch Euclidean:**\n",
    "\n",
    "$$d(x_i, x_j) = \\sqrt{\\sum_{k=1}^{n} (x_{ik} - x_{jk})^2}$$\n",
    "\n",
    "**ğŸ¯ Æ¯u Ä‘iá»ƒm cá»§a KNN Imputation:**\n",
    "- **Preserve relationships**: Giá»¯ nguyÃªn má»‘i quan há»‡ giá»¯a cÃ¡c features\n",
    "- **Non-parametric**: KhÃ´ng giáº£ Ä‘á»‹nh vá» phÃ¢n phá»‘i dá»¯ liá»‡u\n",
    "- **Local patterns**: Táº­n dá»¥ng patterns cá»¥c bá»™ trong dá»¯ liá»‡u\n",
    "- **Multivariate**: Xem xÃ©t táº¥t cáº£ features cÃ¹ng lÃºc\n",
    "\n",
    "**âš™ï¸ CÃ¡c tham sá»‘ quan trá»ng:**\n",
    "- `n_neighbors`: Sá»‘ lÆ°á»£ng neighbors (default=5)\n",
    "- `weights`: 'uniform' hoáº·c 'distance' weighted\n",
    "- `metric`: PhÆ°Æ¡ng phÃ¡p tÃ­nh distance ('nan_euclidean' cho missing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0062f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ DEMO 3: KNN Imputation - Äiá»n táº¥t cáº£ missing values cÃ¹ng lÃºc\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Chuáº©n bá»‹ dá»¯ liá»‡u cho KNN\n",
    "df_knn = df_advanced.copy()\n",
    "print(\"ğŸ“Š Dá»¯ liá»‡u trÆ°á»›c KNN Imputation:\")\n",
    "print(df_knn)\n",
    "\n",
    "# Encode categorical data\n",
    "df_knn['PhÃ²ng ban_encoded'] = le_dept.fit_transform(df_knn['PhÃ²ng ban'].fillna('Unknown'))\n",
    "\n",
    "# Chá»‰ láº¥y cÃ¡c cá»™t sá»‘ cho KNN Imputation\n",
    "numerical_cols = ['Tuá»•i', 'LÆ°Æ¡ng', 'Kinh nghiá»‡m', 'PhÃ²ng ban_encoded']\n",
    "df_numerical = df_knn[numerical_cols].copy()\n",
    "\n",
    "print(f\"\\nğŸ”§ CÃ¡c cá»™t sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng: {numerical_cols}\")\n",
    "print(\"ğŸ“ˆ Ma tráº­n dá»¯ liá»‡u sá»‘ (cÃ³ missing values):\")\n",
    "print(df_numerical)\n",
    "\n",
    "# Hiá»ƒn thá»‹ missing pattern\n",
    "print(f\"\\nğŸ“Š Missing Data Pattern:\")\n",
    "missing_pattern = df_numerical.isnull()\n",
    "print(missing_pattern)\n",
    "\n",
    "# Ãp dá»¥ng KNN Imputation\n",
    "print(f\"\\nğŸ¤– Ãp dá»¥ng KNN Imputation vá»›i k=2 neighbors:\")\n",
    "knn_imputer = KNNImputer(n_neighbors=2, weights='uniform')\n",
    "df_knn_filled = knn_imputer.fit_transform(df_numerical)\n",
    "\n",
    "# Chuyá»ƒn Ä‘á»•i láº¡i thÃ nh DataFrame  \n",
    "df_knn_result = df_knn.copy()\n",
    "df_knn_result['Tuá»•i'] = df_knn_filled[:, 0]\n",
    "df_knn_result['LÆ°Æ¡ng'] = df_knn_filled[:, 1] \n",
    "df_knn_result['Kinh nghiá»‡m'] = df_knn_filled[:, 2]\n",
    "\n",
    "print(f\"\\nâœ… Káº¿t quáº£ sau KNN Imputation:\")\n",
    "result_display = df_knn_result[['TÃªn', 'Tuá»•i', 'LÆ°Æ¡ng', 'PhÃ²ng ban', 'Kinh nghiá»‡m']].copy()\n",
    "result_display['Tuá»•i'] = result_display['Tuá»•i'].round(1)\n",
    "result_display['LÆ°Æ¡ng'] = result_display['LÆ°Æ¡ng'].round(0)\n",
    "print(result_display)\n",
    "\n",
    "# So sÃ¡nh missing values trÆ°á»›c vÃ  sau\n",
    "print(f\"\\nğŸ“Š So sÃ¡nh Missing Values:\")\n",
    "print(f\"TrÆ°á»›c: {df_knn[numerical_cols[:-1]].isnull().sum().sum()} missing values\")\n",
    "print(f\"Sau: {pd.DataFrame(df_knn_filled[:, :-1]).isnull().sum().sum()} missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd82704",
   "metadata": {},
   "source": [
    "#### **C. So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p xá»­ lÃ½ Missing Values**\n",
    "\n",
    "**ğŸ“Š Báº£ng tá»•ng há»£p so sÃ¡nh:**\n",
    "\n",
    "| PhÆ°Æ¡ng phÃ¡p | Äá»™ phá»©c táº¡p | Thá»i gian | Äá»™ chÃ­nh xÃ¡c | PhÃ¹ há»£p vá»›i |\n",
    "|-------------|-------------|-----------|--------------|-------------|\n",
    "| **Mean/Median** | Tháº¥p â­ | Nhanh âš¡âš¡âš¡ | Tháº¥p ğŸ“Š | Dá»¯ liá»‡u Ä‘Æ¡n giáº£n, missing ngáº«u nhiÃªn |\n",
    "| **Mode** | Tháº¥p â­ | Nhanh âš¡âš¡âš¡ | Tháº¥p ğŸ“Š | Categorical data vá»›i Ã­t categories |\n",
    "| **Forward/Backward Fill** | Tháº¥p â­ | Nhanh âš¡âš¡âš¡ | Trung bÃ¬nh ğŸ“ŠğŸ“Š | Time series data |\n",
    "| **Random Forest** | Cao â­â­â­ | Cháº­m âš¡ | Cao ğŸ“ŠğŸ“ŠğŸ“Š | Dá»¯ liá»‡u cÃ³ quan há»‡ phá»©c táº¡p |\n",
    "| **KNN Imputation** | Trung bÃ¬nh â­â­ | Trung bÃ¬nh âš¡âš¡ | Cao ğŸ“ŠğŸ“ŠğŸ“Š | Dá»¯ liá»‡u cÃ³ local patterns |\n",
    "\n",
    "**ğŸ¯ HÆ°á»›ng dáº«n lá»±a chá»n phÆ°Æ¡ng phÃ¡p:**\n",
    "\n",
    "**ğŸ“ˆ Dá»¯ liá»‡u sá»‘ (Numerical):**\n",
    "- **< 5% missing**: Mean/Median\n",
    "- **5-20% missing + cÃ³ correlation**: KNN hoáº·c Random Forest  \n",
    "- **> 20% missing**: CÃ¢n nháº¯c loáº¡i bá» cá»™t hoáº·c thu tháº­p thÃªm dá»¯ liá»‡u\n",
    "\n",
    "**ğŸ·ï¸ Dá»¯ liá»‡u phÃ¢n loáº¡i (Categorical):**\n",
    "- **< 10% missing**: Mode\n",
    "- **> 10% missing + cÃ³ relationship**: Random Forest Classification\n",
    "- **High cardinality**: Táº¡o category \"Unknown\"\n",
    "\n",
    "**â° Dá»¯ liá»‡u thá»i gian (Time Series):**\n",
    "- **Forward fill**: Cho dá»¯ liá»‡u stable (giÃ¡ cá»• phiáº¿u)\n",
    "- **Backward fill**: Cho dá»¯ liá»‡u cÃ³ trend  \n",
    "- **Interpolation**: Cho dá»¯ liá»‡u smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1372d7dc",
   "metadata": {},
   "source": [
    "## Xá»­ lÃ½ dá»¯ liá»‡u trÃ¹ng láº·p (Duplicate Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77a20c",
   "metadata": {},
   "source": [
    "### Hiá»ƒu vá» dá»¯ liá»‡u trÃ¹ng láº·p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf5e95",
   "metadata": {},
   "source": [
    "**ğŸ”„ Dá»¯ liá»‡u trÃ¹ng láº·p lÃ  gÃ¬?**\n",
    "\n",
    "Dá»¯ liá»‡u trÃ¹ng láº·p (*duplicate data*) lÃ  nhá»¯ng **hÃ ng cÃ³ giÃ¡ trá»‹ giá»‘ng há»‡t nhau** trÃªn táº¥t cáº£ hoáº·c má»™t sá»‘ cá»™t nháº¥t Ä‘á»‹nh. Dá»¯ liá»‡u trÃ¹ng láº·p cÃ³ thá»ƒ xuáº¥t hiá»‡n do:\n",
    "\n",
    "- **Lá»—i nháº­p liá»‡u**: NgÆ°á»i dÃ¹ng vÃ´ tÃ¬nh nháº­p cÃ¹ng má»™t thÃ´ng tin nhiá»u láº§n\n",
    "- **Lá»—i há»‡ thá»‘ng**: Há»‡ thá»‘ng ghi nháº­n cÃ¹ng má»™t sá»± kiá»‡n nhiá»u láº§n  \n",
    "- **Gá»™p dá»¯ liá»‡u**: Khi káº¿t há»£p nhiá»u nguá»“n dá»¯ liá»‡u cÃ³ thÃ´ng tin chá»“ng chÃ©o\n",
    "- **Lá»—i thu tháº­p**: Cáº£m biáº¿n hoáº·c thiáº¿t bá»‹ ghi nháº­n dá»¯ liá»‡u bá»‹ láº·p\n",
    "\n",
    "**âš ï¸ TÃ¡c Ä‘á»™ng cá»§a dá»¯ liá»‡u trÃ¹ng láº·p**\n",
    "\n",
    "- **ThiÃªn lá»‡ch phÃ¢n tÃ­ch**: Má»™t quan sÃ¡t Ä‘Æ°á»£c tÃ­nh nhiá»u láº§n, lÃ m mÃ©o mÃ³ káº¿t quáº£\n",
    "- **Giáº£m hiá»‡u quáº£ tÃ­nh toÃ¡n**: Xá»­ lÃ½ dá»¯ liá»‡u thá»«a lÃ m cháº­m thuáº­t toÃ¡n\n",
    "- **TÄƒng kÃ­ch thÆ°á»›c dá»¯ liá»‡u**: LÃ£ng phÃ­ bá»™ nhá»› vÃ  khÃ´ng gian lÆ°u trá»¯\n",
    "- **áº¢nh hÆ°á»Ÿng mÃ´ hÃ¬nh**: Machine learning cÃ³ thá»ƒ há»c sai tá»« dá»¯ liá»‡u láº·p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a749036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Khá»Ÿi táº¡o dá»¯ liá»‡u máº«u\n",
    "duplicate_data = {\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Alice'],\n",
    "    'age': [25, 30, 35, 25],\n",
    "    'city': ['New York', 'Los Angeles', 'Chicago', 'New York']\n",
    "}\n",
    "\n",
    "duplicate_data = pd.DataFrame(duplicate_data)\n",
    "\n",
    "# In dá»¯ liá»‡u máº«u\n",
    "print(duplicate_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed9e8f",
   "metadata": {},
   "source": [
    "### CÃ¡c phÆ°Æ¡ng phÃ¡p phÃ¡t hiá»‡n vÃ  xá»­ lÃ½ dá»¯ liá»‡u trÃ¹ng láº·p trong pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9479d87",
   "metadata": {},
   "source": [
    "Pandas cung cáº¥p cÃ¡c phÆ°Æ¡ng thá»©c Ä‘á»ƒ phÃ¡t hiá»‡n vÃ  xá»­ lÃ½ dá»¯ liá»‡u trÃ¹ng láº·p:\n",
    "\n",
    "| PhÆ°Æ¡ng thá»©c | MÃ´ táº£ | Tráº£ vá» |\n",
    "|-------------|-------|--------|\n",
    "| `duplicated()` | Kiá»ƒm tra tá»«ng hÃ ng cÃ³ bá»‹ trÃ¹ng láº·p khÃ´ng | Boolean Series |\n",
    "| `drop_duplicates()` | Loáº¡i bá» cÃ¡c hÃ ng trÃ¹ng láº·p | DataFrame khÃ´ng trÃ¹ng láº·p |\n",
    "\n",
    "**ğŸ“Š HÃ£y xem cÃ¡ch sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng thá»©c nÃ y:**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bdb44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Khá»Ÿi táº¡o dá»¯ liá»‡u máº«u\n",
    "duplicate_data = {\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Alice'],\n",
    "    'age': [25, 30, 35, 25],\n",
    "    'city': ['New York', 'Los Angeles', 'Chicago', 'New York']\n",
    "}\n",
    "\n",
    "duplicate_data = pd.DataFrame(duplicate_data)\n",
    "\n",
    "# In dá»¯ liá»‡u máº«u\n",
    "print(duplicate_data)\n",
    "\n",
    "# PhÃ¡t hiá»‡n duplicate rows\n",
    "print(\"PhÃ¡t hiá»‡n duplicate rows:\")\n",
    "print(duplicate_data.duplicated())\n",
    "\n",
    "print(\"\\nCÃ¡c hÃ ng bá»‹ duplicate:\")\n",
    "print(duplicate_data[duplicate_data.duplicated()])\n",
    "\n",
    "print(\"\\nÄáº¿m sá»‘ lÆ°á»£ng duplicate:\")\n",
    "print(f\"Tá»•ng sá»‘ duplicate: {duplicate_data.duplicated().sum()}\")\n",
    "\n",
    "print(\"\\n Xá»­ lÃ½ duplicate báº±ng cÃ¡ch loáº¡i bá»:\")\n",
    "print(duplicate_data.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e951b8d",
   "metadata": {},
   "source": [
    "## Biáº¿n Ä‘á»•i vÃ  chuáº©n hÃ³a dá»¯ liá»‡u (Data Transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062b07aa",
   "metadata": {},
   "source": [
    "### Tá»•ng quan vá» biáº¿n Ä‘á»•i dá»¯ liá»‡u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c449fd38",
   "metadata": {},
   "source": [
    "**ğŸ”„ Biáº¿n Ä‘á»•i dá»¯ liá»‡u lÃ  gÃ¬?**\n",
    "\n",
    "Biáº¿n Ä‘á»•i dá»¯ liá»‡u (*data transformation*) lÃ  quÃ¡ trÃ¬nh **chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u** tá»« Ä‘á»‹nh dáº¡ng nÃ y sang Ä‘á»‹nh dáº¡ng khÃ¡c Ä‘á»ƒ:\n",
    "\n",
    "- **Cáº£i thiá»‡n cháº¥t lÆ°á»£ng dá»¯ liá»‡u**: LÃ m cho dá»¯ liá»‡u phÃ¹ há»£p hÆ¡n cho phÃ¢n tÃ­ch\n",
    "- **Chuáº©n hÃ³a thang Ä‘o**: ÄÆ°a cÃ¡c biáº¿n vá» cÃ¹ng má»™t thang Ä‘o\n",
    "- **Giáº£m nhiá»…u**: Loáº¡i bá» cÃ¡c biáº¿n Ä‘á»™ng khÃ´ng mong muá»‘n\n",
    "- **Táº¡o biáº¿n má»›i**: Káº¿t há»£p hoáº·c biáº¿n Ä‘á»•i biáº¿n hiá»‡n cÃ³ Ä‘á»ƒ táº¡o thÃ´ng tin má»›i\n",
    "\n",
    "**ğŸ¯ CÃ¡c má»¥c tiÃªu chÃ­nh:**\n",
    "\n",
    "1. **Normalization**: ÄÆ°a dá»¯ liá»‡u vá» khoáº£ng [0,1]\n",
    "2. **Standardization**: ÄÆ°a dá»¯ liá»‡u vá» phÃ¢n phá»‘i chuáº©n (mean=0, std=1) \n",
    "3. **Scaling**: Äiá»u chá»‰nh thang Ä‘o cho phÃ¹ há»£p\n",
    "4. **Encoding**: Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u phÃ¢n loáº¡i thÃ nh sá»‘\n",
    "\n",
    "**ğŸ“‹ Khi nÃ o cáº§n biáº¿n Ä‘á»•i dá»¯ liá»‡u:**\n",
    "\n",
    "- CÃ¡c biáº¿n cÃ³ **Ä‘Æ¡n vá»‹ Ä‘o khÃ¡c nhau** (VND, USD, kg, cm)\n",
    "- Dá»¯ liá»‡u cÃ³ **pháº¡m vi giÃ¡ trá»‹ chÃªnh lá»‡ch lá»›n** (1-10 vs 1000-10000)\n",
    "- Sá»­ dá»¥ng **thuáº­t toÃ¡n nháº¡y cáº£m vá»›i thang Ä‘o** (KNN, SVM, Neural Networks)\n",
    "- **Cáº£i thiá»‡n hiá»‡u suáº¥t** cá»§a mÃ´ hÃ¬nh machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529a255",
   "metadata": {},
   "source": [
    "### Min-Max Normalization (Chuáº©n hÃ³a Min-Max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05eac5",
   "metadata": {},
   "source": [
    "**ğŸ“ CÃ´ng thá»©c Min-Max Normalization:**\n",
    "\n",
    "$$X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "**ğŸ¯ Äáº·c Ä‘iá»ƒm:**\n",
    "- ÄÆ°a dá»¯ liá»‡u vá» khoáº£ng **[0, 1]**\n",
    "- **Báº£o toÃ n phÃ¢n phá»‘i** gá»‘c cá»§a dá»¯ liá»‡u\n",
    "- **Nháº¡y cáº£m vá»›i outliers** (giÃ¡ trá»‹ ngoáº¡i lai)\n",
    "- PhÃ¹ há»£p khi biáº¿t **giá»›i háº¡n trÃªn vÃ  dÆ°á»›i** cá»§a dá»¯ liá»‡u\n",
    "\n",
    "**ğŸ”§ Sá»­ dá»¥ng `MinMaxScaler` tá»« scikit-learn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import thÆ° viá»‡n cáº§n thiáº¿t cho chuáº©n hÃ³a\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Táº¡o dá»¯ liá»‡u máº«u vá»›i cÃ¡c thang Ä‘o khÃ¡c nhau\n",
    "data_transform = {\n",
    "    'LÆ°Æ¡ng': [15000000, 25000000, 30000000, 18000000, 22000000],\n",
    "    'Tuá»•i': [25, 35, 40, 28, 32],\n",
    "    'Kinh nghiá»‡m': [2, 8, 12, 3, 6]\n",
    "}\n",
    "\n",
    "df_transform = pd.DataFrame(data_transform)\n",
    "print(\"Dá»¯ liá»‡u gá»‘c:\")\n",
    "print(df_transform)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "norm_data_scaled = df_transform.copy()\n",
    "norm_data_scaled[['LÆ°Æ¡ng', 'Tuá»•i', 'Kinh nghiá»‡m']] = scaler.fit_transform(df_transform[['LÆ°Æ¡ng', 'Tuá»•i', 'Kinh nghiá»‡m']])\n",
    "\n",
    "print(\"Min-Max Normalization (0-1):\")\n",
    "print(norm_data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9978ba47",
   "metadata": {},
   "source": [
    "### Standard Scaler (Z-score Standardization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce3d87",
   "metadata": {},
   "source": [
    "**ğŸ“Š CÃ´ng thá»©c Z-score Standardization:**\n",
    "\n",
    "$$Z = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "Trong Ä‘Ã³:\n",
    "- $X$ = giÃ¡ trá»‹ gá»‘c\n",
    "- $\\mu$ = giÃ¡ trá»‹ trung bÃ¬nh (mean)\n",
    "- $\\sigma$ = Ä‘á»™ lá»‡ch chuáº©n (standard deviation)\n",
    "\n",
    "**ğŸ¯ Äáº·c Ä‘iá»ƒm:**\n",
    "- ÄÆ°a dá»¯ liá»‡u vá» **phÃ¢n phá»‘i chuáº©n** vá»›i mean=0, std=1\n",
    "- **KhÃ´ng bá»‹ áº£nh hÆ°á»Ÿng** bá»Ÿi outliers nhiá»u nhÆ° Min-Max\n",
    "- **Báº£o toÃ n thÃ´ng tin** vá» phÃ¢n phá»‘i gá»‘c\n",
    "- PhÃ¹ há»£p vá»›i **cÃ¡c thuáº­t toÃ¡n giáº£ Ä‘á»‹nh phÃ¢n phá»‘i chuáº©n** (Linear Regression, Logistic Regression)\n",
    "\n",
    "**ğŸ”§ Sá»­ dá»¥ng `StandardScaler` tá»« scikit-learn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Táº¡o dá»¯ liá»‡u máº«u vá»›i cÃ¡c thang Ä‘o khÃ¡c nhau\n",
    "data_transform = {\n",
    "    'LÆ°Æ¡ng': [15000000, 25000000, 30000000, 18000000, 22000000],\n",
    "    'Tuá»•i': [25, 35, 40, 28, 32],\n",
    "    'Kinh nghiá»‡m': [2, 8, 12, 3, 6]\n",
    "}\n",
    "\n",
    "# Ãp dá»¥ng Standard Scaler\n",
    "scaler_std = StandardScaler()\n",
    "\n",
    "# Fit vÃ  transform dá»¯ liá»‡u\n",
    "df_standard = df_transform.copy()\n",
    "df_standard[['LÆ°Æ¡ng', 'Tuá»•i', 'Kinh nghiá»‡m']] = scaler_std.fit_transform(\n",
    "    df_transform[['LÆ°Æ¡ng', 'Tuá»•i', 'Kinh nghiá»‡m']]\n",
    ")\n",
    "\n",
    "print(\"Dá»¯ liá»‡u sau Standard Scaling:\")\n",
    "print(df_standard)\n",
    "print(f\"\\nMÃ´ táº£ thá»‘ng kÃª sau standardization:\")\n",
    "print(df_standard.describe().round(4))\n",
    "\n",
    "# Kiá»ƒm tra mean â‰ˆ 0 vÃ  std â‰ˆ 1\n",
    "print(f\"\\nMean cá»§a cÃ¡c cá»™t: {df_standard[['LÆ°Æ¡ng', 'Tuá»•i', 'Kinh nghiá»‡m']].mean().round(4).values}\")\n",
    "print(f\"Std cá»§a cÃ¡c cá»™t: {df_standard[['LÆ°Æ¡ng', 'Tuá»•i', 'Kinh nghiá»‡m']].std().round(4).values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d372e",
   "metadata": {},
   "source": [
    "### Robust Scaler (Sá»­ dá»¥ng Median vÃ  IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d20b9",
   "metadata": {},
   "source": [
    "**ğŸ“ˆ CÃ´ng thá»©c Robust Scaling:**\n",
    "\n",
    "$$X_{robust} = \\frac{X - X_{median}}{IQR}$$\n",
    "\n",
    "Trong Ä‘Ã³:\n",
    "- $X_{median}$ = giÃ¡ trá»‹ trung vá»‹ (median)\n",
    "- $IQR$ = Interquartile Range = Q3 - Q1\n",
    "\n",
    "**ğŸ›¡ï¸ Äáº·c Ä‘iá»ƒm:**\n",
    "- **Ráº¥t bá»n vá»¯ng** (*robust*) trÆ°á»›c outliers\n",
    "- Sá»­ dá»¥ng **median thay vÃ¬ mean**, **IQR thay vÃ¬ std**\n",
    "- **KhÃ´ng bá»‹ mÃ©o** bá»Ÿi cÃ¡c giÃ¡ trá»‹ ngoáº¡i lai\n",
    "- PhÃ¹ há»£p khi dá»¯ liá»‡u cÃ³ **nhiá»u outliers**\n",
    "\n",
    "**ğŸ¯ Khi nÃ o sá»­ dá»¥ng Robust Scaler:**\n",
    "- **Dá»¯ liá»‡u cÃ³ nhiá»u outliers** \n",
    "- **KhÃ´ng muá»‘n loáº¡i bá» outliers** nhÆ°ng váº«n cáº§n chuáº©n hÃ³a\n",
    "- **Dá»¯ liá»‡u khÃ´ng tuÃ¢n theo phÃ¢n phá»‘i chuáº©n**\n",
    "\n",
    "**ğŸ”§ Sá»­ dá»¥ng `RobustScaler` tá»« scikit-learn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a796d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Táº¡o dá»¯ liá»‡u cÃ³ outliers\n",
    "data_with_outliers = {\n",
    "    'LÆ°Æ¡ng': [15000000, 25000000, 30000000, 18000000, 22000000, 200000000],  # outlier: 200M\n",
    "    'Tuá»•i': [25, 35, 40, 28, 32, 22],\n",
    "    'Kinh nghiá»‡m': [2, 8, 12, 3, 6, 1]\n",
    "}\n",
    "\n",
    "df_outliers = pd.DataFrame(data_with_outliers)\n",
    "print(\"Dá»¯ liá»‡u cÃ³ outliers:\")\n",
    "print(df_outliers)\n",
    "print(f\"\\nMÃ´ táº£ thá»‘ng kÃª:\")\n",
    "print(df_outliers.describe())\n",
    "\n",
    "# So sÃ¡nh 3 phÆ°Æ¡ng phÃ¡p scaling\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SO SÃNH CÃC PHÆ¯Æ NG PHÃP SCALING Vá»šI OUTLIERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# MinMax Scaler (bá»‹ áº£nh hÆ°á»Ÿng máº¡nh bá»Ÿi outliers)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaled_minmax = scaler_minmax.fit_transform(df_outliers[['LÆ°Æ¡ng', 'Tuá»•i', 'Kinh nghiá»‡m']])\n",
    "print(\"\\n1. MinMax Scaler (bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi outliers):\")\n",
    "print(pd.DataFrame(scaled_minmax, columns=['LÆ°Æ¡ng', 'Tuá»•i', 'Kinh nghiá»‡m']))\n",
    "\n",
    "# Standard Scaler (bá»‹ áº£nh hÆ°á»Ÿng má»™t pháº§n bá»Ÿi outliers)\n",
    "scaler_std = StandardScaler()\n",
    "scaled_std = scaler_std.fit_transform(df_outliers[['LÆ°Æ¡ng', 'Tuá»•i', 'Kinh nghiá»‡m']])\n",
    "print(\"\\n2. Standard Scaler (bá»‹ áº£nh hÆ°á»Ÿng má»™t pháº§n):\")\n",
    "print(pd.DataFrame(scaled_std, columns=['LÆ°Æ¡ng', 'Tuá»•i', 'Kinh nghiá»‡m']))\n",
    "\n",
    "# Robust Scaler (Ã­t bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi outliers)\n",
    "scaler_robust = RobustScaler()\n",
    "scaled_robust = scaler_robust.fit_transform(df_outliers[['LÆ°Æ¡ng', 'Tuá»•i', 'Kinh nghiá»‡m']])\n",
    "print(\"\\n3. Robust Scaler (bá»n vá»¯ng trÆ°á»›c outliers):\")\n",
    "print(pd.DataFrame(scaled_robust, columns=['LÆ°Æ¡ng', 'Tuá»•i', 'Kinh nghiá»‡m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe95fbe",
   "metadata": {},
   "source": [
    "## Xá»­ lÃ½ chuá»—i kÃ½ tá»± (String Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e544b9d0",
   "metadata": {},
   "source": [
    "### Táº§m quan trá»ng cá»§a viá»‡c xá»­ lÃ½ chuá»—i kÃ½ tá»±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4834c8a",
   "metadata": {},
   "source": [
    "**ğŸ“ Dá»¯ liá»‡u chuá»—i kÃ½ tá»± trong thá»±c táº¿**\n",
    "\n",
    "Dá»¯ liá»‡u chuá»—i kÃ½ tá»± (*string data*) chiáº¿m má»™t pháº§n lá»›n trong cÃ¡c bá»™ dá»¯ liá»‡u thá»±c táº¿:\n",
    "\n",
    "- **TÃªn ngÆ°á»i, Ä‘á»‹a chá»‰**: ThÃ´ng tin cÃ¡ nhÃ¢n\n",
    "- **MÃ´ táº£ sáº£n pháº©m**: Trong thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­\n",
    "- **BÃ¬nh luáº­n, Ä‘Ã¡nh giÃ¡**: Trong phÃ¢n tÃ­ch sentiment\n",
    "- **Danh má»¥c, nhÃ£n**: Dá»¯ liá»‡u phÃ¢n loáº¡i\n",
    "\n",
    "**ğŸ§¹ CÃ¡c váº¥n Ä‘á» thÆ°á»ng gáº·p vá»›i dá»¯ liá»‡u chuá»—i:**\n",
    "\n",
    "1. **KhÃ´ng nháº¥t quÃ¡n vá» Ä‘á»‹nh dáº¡ng**: \"iPhone\", \"iphone\", \"IPHONE\"\n",
    "2. **Khoáº£ng tráº¯ng thá»«a**: \"  Apple  \", \"Apple \"\n",
    "3. **KÃ½ tá»± Ä‘áº·c biá»‡t**: \"email@domain.com\", \"phone: +84-123-456-789\"\n",
    "4. **Viáº¿t táº¯t khÃ¡c nhau**: \"Dr.\", \"Doctor\", \"BS\"\n",
    "5. **Lá»—i chÃ­nh táº£**: \"Compnay\" thay vÃ¬ \"Company\"\n",
    "\n",
    "**ğŸ”§ Pandas String Accessor (`.str`)**\n",
    "\n",
    "Pandas cung cáº¥p **accessor `.str`** cho phÃ©p Ã¡p dá»¥ng cÃ¡c phÆ°Æ¡ng thá»©c xá»­ lÃ½ chuá»—i lÃªn toÃ n bá»™ Series:\n",
    "\n",
    "```python\n",
    "# Thay vÃ¬ lÃ m thá»§ cÃ´ng tá»«ng pháº§n tá»­\n",
    "for i in range(len(df)):\n",
    "    df.loc[i, 'column'] = df.loc[i, 'column'].upper()\n",
    "\n",
    "# Sá»­ dá»¥ng .str accessor\n",
    "df['column'] = df['column'].str.upper()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3bfbce",
   "metadata": {},
   "source": [
    "### CÃ¡c phÆ°Æ¡ng thá»©c cÆ¡ báº£n xá»­ lÃ½ chuá»—i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b337bbe7",
   "metadata": {},
   "source": [
    "**ğŸ“‹ Báº£ng tá»•ng há»£p cÃ¡c phÆ°Æ¡ng thá»©c quan trá»ng:**\n",
    "\n",
    "| PhÆ°Æ¡ng thá»©c | MÃ´ táº£ | VÃ­ dá»¥ |\n",
    "|-------------|-------|-------|\n",
    "| `.str.lower()` | Chuyá»ƒn vá» chá»¯ thÆ°á»ng | `\"HELLO\"` â†’ `\"hello\"` |\n",
    "| `.str.upper()` | Chuyá»ƒn vá» chá»¯ hoa | `\"hello\"` â†’ `\"HELLO\"` |\n",
    "| `.str.title()` | Viáº¿t hoa chá»¯ cÃ¡i Ä‘áº§u | `\"hello world\"` â†’ `\"Hello World\"` |\n",
    "| `.str.strip()` | Loáº¡i bá» khoáº£ng tráº¯ng Ä‘áº§u/cuá»‘i | `\"  hello  \"` â†’ `\"hello\"` |\n",
    "| `.str.replace()` | Thay tháº¿ chuá»—i con | `\"hello\"` â†’ `\"hi\"` |\n",
    "| `.str.contains()` | Kiá»ƒm tra chá»©a chuá»—i con | `\"hello world\"` contains `\"world\"` â†’ `True` |\n",
    "| `.str.startswith()` | Kiá»ƒm tra báº¯t Ä‘áº§u báº±ng | `\"hello\"` startswith `\"he\"` â†’ `True` |\n",
    "| `.str.endswith()` | Kiá»ƒm tra káº¿t thÃºc báº±ng | `\"hello\"` endswith `\"lo\"` â†’ `True` |\n",
    "| `.str.len()` | Äá»™ dÃ i chuá»—i | `\"hello\"` â†’ `5` |\n",
    "| `.str.split()` | TÃ¡ch chuá»—i | `\"a,b,c\"` â†’ `[\"a\", \"b\", \"c\"]` |\n",
    "\n",
    "**ğŸ”¥ HÃ£y xem cÃ¡c vÃ­ dá»¥ thá»±c táº¿:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c701e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chuyá»ƒn Ä‘á»•i kiá»ƒu dá»¯ liá»‡u\n",
    "sample_data = pd.DataFrame({\n",
    "    'id': ['1', '2', '3', '4', '5'],\n",
    "    'score': ['85.5', '90.0', '78.5', '92.0', '88.5'],\n",
    "    'date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'],\n",
    "    'category': ['A', 'B', 'A', 'C', 'B']\n",
    "})\n",
    "\n",
    "print(\"Dá»¯ liá»‡u gá»‘c vÃ  kiá»ƒu dá»¯ liá»‡u:\")\n",
    "print(sample_data.dtypes)\n",
    "print()\n",
    "print(sample_data)\n",
    "\n",
    "# Chuyá»ƒn Ä‘á»•i kiá»ƒu dá»¯ liá»‡u\n",
    "sample_data['id'] = sample_data['id'].astype('int64')\n",
    "sample_data['score'] = sample_data['score'].astype('float64')\n",
    "sample_data['date'] = pd.to_datetime(sample_data['date'])\n",
    "sample_data['category'] = sample_data['category'].astype('category')\n",
    "\n",
    "print(\"\\nSau khi chuyá»ƒn Ä‘á»•i kiá»ƒu dá»¯ liá»‡u:\")\n",
    "print(sample_data.dtypes)\n",
    "print()\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c64432",
   "metadata": {},
   "source": [
    "### Normalization vÃ  Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d22fbc",
   "metadata": {},
   "source": [
    "Normalization vÃ  standardization lÃ  cÃ¡c ká»¹ thuáº­t quan trá»ng Ä‘á»ƒ Ä‘Æ°a dá»¯ liá»‡u vá» cÃ¹ng má»™t thang Ä‘o, Ä‘áº·c biá»‡t há»¯u Ã­ch cho machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbffc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Táº¡o dá»¯ liá»‡u máº«u cho normalization\n",
    "norm_data = pd.DataFrame({\n",
    "    'height': [150, 160, 170, 180, 190],  # cm\n",
    "    'weight': [50, 60, 70, 80, 90],       # kg  \n",
    "    'income': [30000, 45000, 60000, 75000, 90000]  # VND/month\n",
    "})\n",
    "\n",
    "print(\"Dá»¯ liá»‡u gá»‘c:\")\n",
    "print(norm_data)\n",
    "print(\"\\nMÃ´ táº£ thá»‘ng kÃª:\")\n",
    "print(norm_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2fc07",
   "metadata": {},
   "source": [
    "### Regular Expressions (Regex) cho xá»­ lÃ½ chuá»—i nÃ¢ng cao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf5f680",
   "metadata": {},
   "source": [
    "**ğŸ” Regular Expressions lÃ  gÃ¬?**\n",
    "\n",
    "Regular Expressions (*regex*) lÃ  **ngÃ´n ngá»¯ pattern matching** máº¡nh máº½ Ä‘á»ƒ tÃ¬m kiáº¿m vÃ  thao tÃ¡c vá»›i chuá»—i kÃ½ tá»±:\n",
    "\n",
    "**ğŸ“‹ CÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t thÆ°á»ng dÃ¹ng:**\n",
    "\n",
    "| Pattern | MÃ´ táº£ | VÃ­ dá»¥ |\n",
    "|---------|-------|-------|\n",
    "| `\\d` | Sá»‘ (0-9) | `\"abc123\"` â†’ tÃ¬m tháº¥y `\"123\"` |\n",
    "| `\\w` | KÃ½ tá»± tá»« (a-z, A-Z, 0-9, _) | `\"hello_123\"` â†’ tÃ¬m tháº¥y táº¥t cáº£ |\n",
    "| `\\s` | Khoáº£ng tráº¯ng | `\"a b c\"` â†’ tÃ¬m tháº¥y 2 spaces |\n",
    "| `+` | 1 hoáº·c nhiá»u láº§n | `\\d+` â†’ `\"123\"` (nhiá»u sá»‘ liÃªn tiáº¿p) |\n",
    "| `*` | 0 hoáº·c nhiá»u láº§n | `\\d*` â†’ cÃ³ thá»ƒ khÃ´ng cÃ³ sá»‘ |\n",
    "| `?` | 0 hoáº·c 1 láº§n | `\\d?` â†’ tá»‘i Ä‘a 1 sá»‘ |\n",
    "| `[]` | NhÃ³m kÃ½ tá»± | `[0-9]` tÆ°Æ¡ng Ä‘Æ°Æ¡ng `\\d` |\n",
    "| `^` | Báº¯t Ä‘áº§u chuá»—i | `^Hello` â†’ chuá»—i báº¯t Ä‘áº§u báº±ng \"Hello\" |\n",
    "| `$` | Káº¿t thÃºc chuá»—i | `world$` â†’ chuá»—i káº¿t thÃºc báº±ng \"world\" |\n",
    "\n",
    "**ğŸ”§ Sá»­ dá»¥ng regex vá»›i pandas `.str` accessor:**\n",
    "\n",
    "- `.str.contains(pattern)` - kiá»ƒm tra chá»©a pattern\n",
    "- `.str.extract(pattern)` - trÃ­ch xuáº¥t groups tá»« pattern  \n",
    "- `.str.replace(pattern, replacement, regex=True)` - thay tháº¿ vá»›i regex\n",
    "- `.str.findall(pattern)` - tÃ¬m táº¥t cáº£ matches\n",
    "\n",
    "**ğŸ“± VÃ­ dá»¥ thá»±c táº¿: Xá»­ lÃ½ sá»‘ Ä‘iá»‡n thoáº¡i, email, mÃ£ zip**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce829d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Táº¡o dá»¯ liá»‡u máº«u vá»›i cÃ¡c patterns phá»©c táº¡p\n",
    "data_regex = {\n",
    "    'text': [\n",
    "        'LiÃªn há»‡: 0123-456-789 hoáº·c email: john@gmail.com',\n",
    "        'SDT: +84 98 765 4321, Ä‘á»‹a chá»‰: 123 LÃª Lá»£i, Q1, TP.HCM', \n",
    "        'Phone: (024) 3825-7863, email: info@company.vn',\n",
    "        'Mobile: 0987654321, website: https://example.com',\n",
    "        'Hotline: 1900-1234, fax: (028) 3829-5678'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_regex = pd.DataFrame(data_regex)\n",
    "print(\"Dá»¯ liá»‡u gá»‘c:\")\n",
    "print(df_regex)\n",
    "\n",
    "# 1. TrÃ­ch xuáº¥t sá»‘ Ä‘iá»‡n thoáº¡i\n",
    "print(\"\\n1. TrÃ­ch xuáº¥t sá»‘ Ä‘iá»‡n thoáº¡i:\")\n",
    "phone_pattern = r'(\\+?\\d{1,3}[\\s\\-]?)?\\(?0?\\d{2,3}\\)?[\\s\\-]?\\d{3,4}[\\s\\-]?\\d{3,4}'\n",
    "df_regex['phone'] = df_regex['text'].str.extract(phone_pattern, expand=False)\n",
    "print(df_regex[['text', 'phone']])\n",
    "\n",
    "# 2. TrÃ­ch xuáº¥t email\n",
    "print(\"\\n2. TrÃ­ch xuáº¥t email:\")\n",
    "email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n",
    "df_regex['email'] = df_regex['text'].str.extract(email_pattern, expand=False)\n",
    "print(df_regex[['text', 'email']])\n",
    "\n",
    "# 3. Kiá»ƒm tra chá»©a website\n",
    "print(\"\\n3. Kiá»ƒm tra chá»©a website/URL:\")\n",
    "url_pattern = r'https?://[^\\s]+'\n",
    "df_regex['has_url'] = df_regex['text'].str.contains(url_pattern, regex=True)\n",
    "print(df_regex[['text', 'has_url']])\n",
    "\n",
    "# 4. LÃ m sáº¡ch vÃ  chuáº©n hÃ³a sá»‘ Ä‘iá»‡n thoáº¡i\n",
    "print(\"\\n4. Chuáº©n hÃ³a sá»‘ Ä‘iá»‡n thoáº¡i:\")\n",
    "def clean_phone(text):\n",
    "    # TÃ¬m táº¥t cáº£ sá»‘ Ä‘iá»‡n thoáº¡i\n",
    "    phones = str(text).replace('nan', '')\n",
    "    # Chá»‰ giá»¯ láº¡i sá»‘\n",
    "    cleaned = ''.join(filter(str.isdigit, phones))\n",
    "    # Format láº¡i náº¿u cÃ³ Ä‘á»§ sá»‘\n",
    "    if len(cleaned) >= 10:\n",
    "        if cleaned.startswith('84'):\n",
    "            return f\"+84-{cleaned[2:5]}-{cleaned[5:8]}-{cleaned[8:]}\"\n",
    "        elif cleaned.startswith('0'):\n",
    "            return f\"{cleaned[:4]}-{cleaned[4:7]}-{cleaned[7:]}\"\n",
    "    return cleaned if cleaned else None\n",
    "\n",
    "df_regex['phone_clean'] = df_regex['phone'].apply(clean_phone)\n",
    "print(df_regex[['phone', 'phone_clean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b32509",
   "metadata": {},
   "source": [
    "## Xá»­ lÃ½ dá»¯ liá»‡u phÃ¢n loáº¡i (Categorical Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0020f9e7",
   "metadata": {},
   "source": [
    "### Hiá»ƒu vá» dá»¯ liá»‡u phÃ¢n loáº¡i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30a323b",
   "metadata": {},
   "source": [
    "**ğŸ·ï¸ Dá»¯ liá»‡u phÃ¢n loáº¡i lÃ  gÃ¬?**\n",
    "\n",
    "Dá»¯ liá»‡u phÃ¢n loáº¡i (*categorical data*) lÃ  loáº¡i dá»¯ liá»‡u cÃ³ **sá»‘ lÆ°á»£ng giÃ¡ trá»‹ há»¯u háº¡n** vÃ  thÆ°á»ng Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng **nhÃ£n hoáº·c tÃªn**:\n",
    "\n",
    "**ğŸ“Š CÃ¡c loáº¡i dá»¯ liá»‡u phÃ¢n loáº¡i:**\n",
    "\n",
    "1. **Nominal** (Danh nghÄ©a): KhÃ´ng cÃ³ thá»© tá»±\n",
    "   - Giá»›i tÃ­nh: Nam, Ná»¯, KhÃ¡c\n",
    "   - MÃ u sáº¯c: Äá», Xanh, VÃ ng\n",
    "   - Quá»‘c gia: Viá»‡t Nam, Má»¹, Nháº­t Báº£n\n",
    "\n",
    "2. **Ordinal** (Thá»© tá»±): CÃ³ thá»© tá»± Ã½ nghÄ©a\n",
    "   - Há»c vá»‹: Cá»­ nhÃ¢n < Tháº¡c sÄ© < Tiáº¿n sÄ©\n",
    "   - ÄÃ¡nh giÃ¡: KÃ©m < Trung bÃ¬nh < Tá»‘t < Xuáº¥t sáº¯c\n",
    "   - KÃ­ch cá»¡: S < M < L < XL\n",
    "\n",
    "**ğŸ”§ Xá»­ lÃ½ dá»¯ liá»‡u phÃ¢n loáº¡i trong pandas:**\n",
    "\n",
    "- **Kiá»ƒu `category`**: Pandas cÃ³ kiá»ƒu dá»¯ liá»‡u chuyÃªn dá»¥ng cho categorical data\n",
    "- **Memory efficient**: Tiáº¿t kiá»‡m bá»™ nhá»› khi cÃ³ nhiá»u giÃ¡ trá»‹ láº·p láº¡i\n",
    "- **Performance**: TÄƒng tá»‘c cÃ¡c phÃ©p toÃ¡n groupby vÃ  merge\n",
    "- **Validation**: Kiá»ƒm soÃ¡t cÃ¡c giÃ¡ trá»‹ há»£p lá»‡\n",
    "\n",
    "**âš™ï¸ Khi nÃ o sá»­ dá»¥ng kiá»ƒu `category`:**\n",
    "\n",
    "- Cá»™t cÃ³ **Ã­t giÃ¡ trá»‹ duy nháº¥t** so vá»›i tá»•ng sá»‘ hÃ ng\n",
    "- **Nhiá»u giÃ¡ trá»‹ láº·p láº¡i** (high cardinality)\n",
    "- Muá»‘n **kiá»ƒm soÃ¡t cÃ¡c giÃ¡ trá»‹** cÃ³ thá»ƒ xuáº¥t hiá»‡n\n",
    "- Cáº§n **tá»‘i Æ°u hÃ³a bá»™ nhá»›** vÃ  hiá»‡u suáº¥t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc6ba5",
   "metadata": {},
   "source": [
    "### Label Encoding - MÃ£ hÃ³a nhÃ£n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd7819",
   "metadata": {},
   "source": [
    "**ğŸ”¢ Label Encoding lÃ  gÃ¬?**\n",
    "\n",
    "Label Encoding lÃ  ká»¹ thuáº­t chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u phÃ¢n loáº¡i thÃ nh **sá»‘ nguyÃªn tuáº§n tá»±**:\n",
    "\n",
    "- `\"Apple\"` â†’ `0`\n",
    "- `\"Banana\"` â†’ `1` \n",
    "- `\"Cherry\"` â†’ `2`\n",
    "\n",
    "**âœ… Æ¯u Ä‘iá»ƒm:**\n",
    "- **ÄÆ¡n giáº£n**: Dá»… hiá»ƒu vÃ  triá»ƒn khai\n",
    "- **Tiáº¿t kiá»‡m bá»™ nhá»›**: Chá»‰ cáº§n 1 cá»™t\n",
    "- **PhÃ¹ há»£p vá»›i dá»¯ liá»‡u ordinal**: Báº£o toÃ n thá»© tá»±\n",
    "\n",
    "**âŒ NhÆ°á»£c Ä‘iá»ƒm:**\n",
    "- **Táº¡o thá»© tá»± giáº£ táº¡o**: Apple < Banana < Cherry (khÃ´ng Ä‘Ãºng)\n",
    "- **KhÃ´ng phÃ¹ há»£p vá»›i nominal data**: CÃ¡c thuáº­t toÃ¡n cÃ³ thá»ƒ hiá»ƒu sai quan há»‡\n",
    "- **Bias trong mÃ´ hÃ¬nh**: GiÃ¡ trá»‹ lá»›n hÆ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  \"quan trá»ng\" hÆ¡n\n",
    "\n",
    "**ğŸ¯ Khi nÃ o sá»­ dá»¥ng Label Encoding:**\n",
    "- **Dá»¯ liá»‡u ordinal** cÃ³ thá»© tá»± tá»± nhiÃªn\n",
    "- **Tree-based algorithms** (Decision Tree, Random Forest) - Ã­t bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi thá»© tá»±\n",
    "- **Target variable** trong classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e7de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Táº¡o dá»¯ liá»‡u categorical Ä‘á»ƒ demo One-Hot Encoding\n",
    "data_categorical = {\n",
    "    'TÃªn': ['An', 'BÃ¬nh', 'Chi', 'DÅ©ng', 'Eva'],\n",
    "    'PhÃ²ng ban': ['IT', 'Marketing', 'IT', 'HR', 'Marketing'],\n",
    "    'TrÃ¬nh Ä‘á»™': ['Cá»­ nhÃ¢n', 'Tháº¡c sÄ©', 'Cá»­ nhÃ¢n', 'Tiáº¿n sÄ©', 'Tháº¡c sÄ©'],\n",
    "    'ThÃ nh phá»‘': ['HÃ  Ná»™i', 'TP.HCM', 'HÃ  Ná»™i', 'ÄÃ  Náºµng', 'TP.HCM']\n",
    "}\n",
    "\n",
    "df_categorical = pd.DataFrame(data_categorical)\n",
    "print(\"Dá»¯ liá»‡u categorical gá»‘c:\")\n",
    "print(df_categorical)\n",
    "\n",
    "print(\"Sá»­ dá»¥ng Label Encoding Ä‘á»‘i vá»›i dá»¯ liá»‡u categorical:\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Khá»Ÿi táº¡o LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Ãp dá»¥ng Label Encoding cho tá»«ng cá»™t categorical\n",
    "for col in ['PhÃ²ng ban', 'TrÃ¬nh Ä‘á»™', 'ThÃ nh phá»‘']:\n",
    "    df_categorical[col] = label_encoder.fit_transform(df_categorical[col])\n",
    "    print(f\"CÃ¡c categoricals Ä‘Ã£ Ä‘Æ°á»£c mÃ£ hÃ³a Ä‘á»‘i vá»›i {col}: {label_encoder.classes_}\")\n",
    "\n",
    "print(\"Káº¿t quáº£ Label Encoding:\")\n",
    "print(df_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7abc1c",
   "metadata": {},
   "source": [
    "### **One-Hot Encoding - MÃ£ hÃ³a One-Hot**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47859e9",
   "metadata": {},
   "source": [
    "**ğŸ”¥ One-Hot Encoding lÃ  gÃ¬?**\n",
    "\n",
    "One-Hot Encoding táº¡o ra **binary columns** cho má»—i category:\n",
    "\n",
    "**VÃ­ dá»¥:** `[\"Apple\", \"Banana\", \"Cherry\"]` â†’ \n",
    "\n",
    "| Apple | Banana | Cherry |\n",
    "|-------|--------|--------|\n",
    "| 1     | 0      | 0      |\n",
    "| 0     | 1      | 0      |\n",
    "| 0     | 0      | 1      |\n",
    "\n",
    "**âœ… Æ¯u Ä‘iá»ƒm:**\n",
    "- **KhÃ´ng táº¡o thá»© tá»± giáº£ táº¡o**: Táº¥t cáº£ categories Ä‘á»u bÃ¬nh Ä‘áº³ng\n",
    "- **PhÃ¹ há»£p vá»›i nominal data**: Apple â‰  Banana â‰  Cherry\n",
    "- **Hoáº¡t Ä‘á»™ng tá»‘t** vá»›i háº§u háº¿t machine learning algorithms\n",
    "- **TrÃ¡nh bias**: KhÃ´ng cÃ³ category nÃ o Ä‘Æ°á»£c coi lÃ  \"quan trá»ng\" hÆ¡n\n",
    "\n",
    "**âŒ NhÆ°á»£c Ä‘iá»ƒm:**\n",
    "- **Curse of dimensionality**: TÄƒng sá»‘ lÆ°á»£ng features Ä‘Ã¡ng ká»ƒ  \n",
    "- **Sparse matrix**: Nhiá»u giÃ¡ trá»‹ 0, tá»‘n bá»™ nhá»›\n",
    "- **Multicollinearity**: CÃ¡c cá»™t cÃ³ correlation vá»›i nhau\n",
    "\n",
    "**ğŸ¯ Khi nÃ o sá»­ dá»¥ng One-Hot Encoding:**\n",
    "- **Dá»¯ liá»‡u nominal** khÃ´ng cÃ³ thá»© tá»± tá»± nhiÃªn\n",
    "- **Ãt categories** (< 10-15 giÃ¡ trá»‹ duy nháº¥t)\n",
    "- **Linear algorithms** (Linear/Logistic Regression, SVM)\n",
    "- **Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f7870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Táº¡o dá»¯ liá»‡u categorical Ä‘á»ƒ demo One-Hot Encoding\n",
    "data_categorical = {\n",
    "    'TÃªn': ['An', 'BÃ¬nh', 'Chi', 'DÅ©ng', 'Eva'],\n",
    "    'PhÃ²ng ban': ['IT', 'Marketing', 'IT', 'HR', 'Marketing'],\n",
    "    'TrÃ¬nh Ä‘á»™': ['Cá»­ nhÃ¢n', 'Tháº¡c sÄ©', 'Cá»­ nhÃ¢n', 'Tiáº¿n sÄ©', 'Tháº¡c sÄ©'],\n",
    "    'ThÃ nh phá»‘': ['HÃ  Ná»™i', 'TP.HCM', 'HÃ  Ná»™i', 'ÄÃ  Náºµng', 'TP.HCM']\n",
    "}\n",
    "\n",
    "df_categorical = pd.DataFrame(data_categorical)\n",
    "print(\"Dá»¯ liá»‡u categorical gá»‘c:\")\n",
    "print(df_categorical)\n",
    "\n",
    "print(\"PHÆ¯Æ NG PHÃP 1: Sá»¬ Dá»¤NG pandas.get_dummies()\")\n",
    "\n",
    "# PhÆ°Æ¡ng phÃ¡p 1: Sá»­ dá»¥ng pandas.get_dummies()\n",
    "df_onehot_pandas = pd.get_dummies(df_categorical, \n",
    "                                  columns=['PhÃ²ng ban', 'TrÃ¬nh Ä‘á»™', 'ThÃ nh phá»‘'],\n",
    "                                  prefix=['PB', 'TD', 'TP'])\n",
    "\n",
    "print(\"Káº¿t quáº£ One-Hot Encoding vá»›i pandas:\")\n",
    "print(df_onehot_pandas)\n",
    "\n",
    "print(f\"\\nSá»‘ cá»™t trÆ°á»›c: {len(df_categorical.columns)}\")\n",
    "print(f\"Sá»‘ cá»™t sau: {len(df_onehot_pandas.columns)}\")\n",
    "\n",
    "print(\"PHÆ¯Æ NG PHÃP 2: Sá»¬ Dá»¤NG sklearn.OneHotEncoder\")\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# PhÆ°Æ¡ng phÃ¡p 2: Sá»­ dá»¥ng sklearn OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' Ä‘á»ƒ trÃ¡nh multicollinearity\n",
    "\n",
    "# Chá»‰ encode cÃ¡c cá»™t categorical (bá» qua cá»™t 'TÃªn')\n",
    "categorical_cols = ['PhÃ²ng ban', 'TrÃ¬nh Ä‘á»™', 'ThÃ nh phá»‘']\n",
    "encoded_data = encoder.fit_transform(df_categorical[categorical_cols])\n",
    "\n",
    "# Táº¡o tÃªn cá»™t cho káº¿t quáº£\n",
    "feature_names = encoder.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Táº¡o DataFrame má»›i\n",
    "df_onehot_sklearn = pd.DataFrame(encoded_data, columns=feature_names)\n",
    "df_onehot_sklearn = pd.concat([df_categorical[['TÃªn']], df_onehot_sklearn], axis=1)\n",
    "\n",
    "print(\"Káº¿t quáº£ One-Hot Encoding vá»›i sklearn:\")\n",
    "print(df_onehot_sklearn)\n",
    "\n",
    "print(f\"\\nLÆ°u Ã½: sklearn vá»›i drop='first' giáº£m sá»‘ cá»™t Ä‘á»ƒ trÃ¡nh multicollinearity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011775ca",
   "metadata": {},
   "source": [
    "## CÃ¢u há»i Ã´n táº­p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750156b9",
   "metadata": {},
   "source": [
    "**ğŸ“ HÃ£y tráº£ lá»i cÃ¡c cÃ¢u há»i sau Ä‘á»ƒ kiá»ƒm tra hiá»ƒu biáº¿t cá»§a báº¡n:**\n",
    "\n",
    "| **PhÆ°Æ¡ng thá»©c nÃ o dÃ¹ng Ä‘á»ƒ phÃ¡t hiá»‡n dá»¯ liá»‡u thiáº¿u trong pandas?** | |\n",
    "|---|---|\n",
    "| `isna()` hoáº·c `isnull()` | |\n",
    "| `missing()` | |\n",
    "| `empty()` | |\n",
    "| `nan_check()` | |\n",
    "\n",
    "| **PhÆ°Æ¡ng thá»©c `fillna()` Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ lÃ m gÃ¬?** | |\n",
    "|---|---|\n",
    "| Loáº¡i bá» dá»¯ liá»‡u thiáº¿u | |\n",
    "| Thay tháº¿ dá»¯ liá»‡u thiáº¿u | |\n",
    "| PhÃ¡t hiá»‡n dá»¯ liá»‡u thiáº¿u | |\n",
    "| Äáº¿m dá»¯ liá»‡u thiáº¿u | |\n",
    "\n",
    "| **MinMaxScaler Ä‘Æ°a dá»¯ liá»‡u vá» khoáº£ng giÃ¡ trá»‹ nÃ o?** | |\n",
    "|---|---|\n",
    "| [-1, 1] | |\n",
    "| [0, 1] | |\n",
    "| [0, 100] | |\n",
    "| [-100, 100] | |\n",
    "\n",
    "| **PhÆ°Æ¡ng thá»©c nÃ o dÃ¹ng Ä‘á»ƒ loáº¡i bá» hÃ ng trÃ¹ng láº·p?** | |\n",
    "|---|---|\n",
    "| `remove_duplicates()` | |\n",
    "| `drop_duplicates()` | |\n",
    "| `delete_duplicates()` | |\n",
    "| `unique()` | |\n",
    "\n",
    "| **Trong pandas, Ä‘á»ƒ chuyá»ƒn chuá»—i vá» chá»¯ thÆ°á»ng ta sá»­ dá»¥ng?** | |\n",
    "|---|---|\n",
    "| `.str.lowercase()` | |\n",
    "| `.str.lower()` | |\n",
    "| `.str.downcase()` | |\n",
    "| `.str.small()` | |\n",
    "\n",
    "| **Label Encoding phÃ¹ há»£p nháº¥t vá»›i loáº¡i dá»¯ liá»‡u nÃ o?** | |\n",
    "|---|---|\n",
    "| Dá»¯ liá»‡u sá»‘ liÃªn tá»¥c | |\n",
    "| Dá»¯ liá»‡u nominal | |\n",
    "| Dá»¯ liá»‡u ordinal | |\n",
    "| Dá»¯ liá»‡u thá»i gian | |\n",
    "\n",
    "| **StandardScaler chuáº©n hÃ³a dá»¯ liá»‡u cÃ³ Mean vÃ  Standard Deviation lÃ  bao nhiÃªu?** | |\n",
    "|---|---|\n",
    "| Mean=1, Std=0 | |\n",
    "| Mean=0, Std=1 | |\n",
    "| Mean=0.5, Std=0.5 | |\n",
    "| Mean=100, Std=10 | |\n",
    "\n",
    "| **Khi nÃ o nÃªn sá»­ dá»¥ng RobustScaler thay vÃ¬ MinMaxScaler?** | |\n",
    "|---|---|\n",
    "| Khi dá»¯ liá»‡u cÃ³ nhiá»u outliers | |\n",
    "| Khi dá»¯ liá»‡u Ä‘Ã£ chuáº©n hÃ³a | |\n",
    "| Khi dá»¯ liá»‡u lÃ  categorical | |\n",
    "| Khi dá»¯ liá»‡u cÃ³ kÃ­ch thÆ°á»›c nhá» | |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
