{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80bc3a71",
   "metadata": {},
   "source": [
    "# üìä B√†i t·∫≠p th·ª±c h√†nh: L√†m s·∫°ch v√† chu·∫©n b·ªã d·ªØ li·ªáu\n",
    "\n",
    "## üéØ M·ª•c ti√™u b√†i t·∫≠p\n",
    "\n",
    "Sau khi ho√†n th√†nh c√°c b√†i t·∫≠p n√†y, b·∫°n s·∫Ω c√≥ th·ªÉ:\n",
    "\n",
    "‚úÖ **Th·ª±c h√†nh ph√°t hi·ªán v√† x·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu** trong c√°c t√¨nh hu·ªëng kinh t·∫ø th·ª±c t·∫ø  \n",
    "‚úÖ **L√†m s·∫°ch d·ªØ li·ªáu tr√πng l·∫∑p** t·ª´ kh·∫£o s√°t kh√°ch h√†ng  \n",
    "‚úÖ **Chu·∫©n h√≥a d·ªØ li·ªáu** ƒë·ªÉ ph√π h·ª£p v·ªõi ph√¢n t√≠ch  \n",
    "‚úÖ **X·ª≠ l√Ω d·ªØ li·ªáu chu·ªói k√Ω t·ª±** t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau  \n",
    "‚úÖ **M√£ h√≥a d·ªØ li·ªáu ph√¢n lo·∫°i** cho machine learning  \n",
    "\n",
    "---\n",
    "\n",
    "## üìã C·∫•u tr√∫c b√†i t·∫≠p\n",
    "\n",
    "**Ph·∫ßn 1: X·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu** (3 b√†i)\n",
    "- B√†i 1.1: Ph√°t hi·ªán d·ªØ li·ªáu thi·∫øu\n",
    "- B√†i 1.2: Lo·∫°i b·ªè d·ªØ li·ªáu thi·∫øu (dropna)\n",
    "- B√†i 1.3: Thay th·∫ø d·ªØ li·ªáu thi·∫øu (fillna)\n",
    "\n",
    "**Ph·∫ßn 2: X·ª≠ l√Ω d·ªØ li·ªáu tr√πng l·∫∑p** (1 b√†i)\n",
    "- B√†i 2.1: Ph√°t hi·ªán v√† x·ª≠ l√Ω duplicates\n",
    "\n",
    "**Ph·∫ßn 3: Bi·∫øn ƒë·ªïi v√† chu·∫©n h√≥a** (3 b√†i)\n",
    "- B√†i 3.1: Min-Max Normalization\n",
    "- B√†i 3.2: Z-score Standardization  \n",
    "- B√†i 3.3: Robust Scaler v√† outliers\n",
    "\n",
    "**Ph·∫ßn 4: X·ª≠ l√Ω chu·ªói k√Ω t·ª±** (2 b√†i)\n",
    "- B√†i 4.1: X·ª≠ l√Ω chu·ªói c∆° b·∫£n\n",
    "- B√†i 4.2: Regular Expressions\n",
    "\n",
    "**Ph·∫ßn 5: X·ª≠ l√Ω d·ªØ li·ªáu ph√¢n lo·∫°i** (2 b√†i)\n",
    "- B√†i 5.1: Label Encoding\n",
    "- B√†i 5.2: One-Hot Encoding\n",
    "\n",
    "**Ph·∫ßn 6: B√†i t·∫≠p t·ªïng h·ª£p** (1 b√†i)\n",
    "- B√†i 6.1: D·ª± √°n l√†m s·∫°ch d·ªØ li·ªáu ho√†n ch·ªânh\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è L∆∞u √Ω quan tr·ªçng\n",
    "\n",
    "> **üí° Cho sinh vi√™n Kinh t·∫ø:** C√°c b√†i t·∫≠p n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø d·ª±a tr√™n c√°c t√¨nh hu·ªëng th·ª±c t·∫ø trong kinh doanh v√† nghi√™n c·ª©u kinh t·∫ø.\n",
    "\n",
    "> **üîß T∆∞∆°ng th√≠ch:** Notebook n√†y ho·∫°t ƒë·ªông t·ªët tr√™n c·∫£ **Jupyter Notebook**, **JupyterLab**, v√† **Google Colab**.\n",
    "\n",
    "> **üìö Tham kh·∫£o:** H√£y xem l·∫°i **Lecture 5** tr∆∞·ªõc khi l√†m b√†i t·∫≠p ƒë·ªÉ hi·ªÉu r√µ l√Ω thuy·∫øt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6595e29e",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng\n",
    "\n",
    "Tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu l√†m b√†i t·∫≠p, h√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt:\n",
    "\n",
    "**üì¶ C√†i ƒë·∫∑t th∆∞ vi·ªán (ch·∫°y cell n√†y n·∫øu b·∫°n ƒëang s·ª≠ d·ª•ng Google Colab):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab815a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t th∆∞ vi·ªán cho Google Colab (b·ªè qua n·∫øu ƒë√£ c√†i ƒë·∫∑t)\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, LabelEncoder, OneHotEncoder\n",
    "    print(\"‚úÖ T·∫•t c·∫£ th∆∞ vi·ªán ƒë√£ s·∫µn s√†ng!\")\n",
    "    print(\"üöÄ B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu l√†m b√†i t·∫≠p!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Thi·∫øu th∆∞ vi·ªán: {e}\")\n",
    "    print(\"üîß ƒêang c√†i ƒë·∫∑t...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    # C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "    packages = ['pandas', 'numpy', 'scikit-learn']\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "    \n",
    "    print(\"‚úÖ C√†i ƒë·∫∑t ho√†n t·∫•t! Vui l√≤ng restart kernel v√† ch·∫°y l·∫°i cell n√†y.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bacbf7",
   "metadata": {},
   "source": [
    "# üîç Ph·∫ßn 1: X·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu (Missing Data)\n",
    "\n",
    "## B√†i 1.1 - Ph√°t hi·ªán d·ªØ li·ªáu thi·∫øu\n",
    "\n",
    "### üìä T√¨nh hu·ªëng th·ª±c t·∫ø\n",
    "B·∫°n l√† nh√¢n vi√™n ph√¢n t√≠ch d·ªØ li·ªáu t·∫°i m·ªôt c√¥ng ty. B·ªô ph·∫≠n HR cung c·∫•p cho b·∫°n d·ªØ li·ªáu nh√¢n vi√™n ƒë·ªÉ ph√¢n t√≠ch v·ªÅ m·ª©c l∆∞∆°ng v√† hi·ªáu su·∫•t l√†m vi·ªác. Tuy nhi√™n, d·ªØ li·ªáu c√≥ m·ªôt s·ªë th√¥ng tin b·ªã thi·∫øu.\n",
    "\n",
    "### üéØ Nhi·ªám v·ª• c·ªßa b·∫°n:\n",
    "1. **T·∫°o DataFrame** v·ªõi d·ªØ li·ªáu nh√¢n vi√™n c√≥ missing values\n",
    "2. **Ph√°t hi·ªán** d·ªØ li·ªáu thi·∫øu b·∫±ng c√°c ph∆∞∆°ng th·ª©c pandas\n",
    "3. **ƒê·∫øm v√† ph√¢n t√≠ch** t·ª∑ l·ªá d·ªØ li·ªáu thi·∫øu\n",
    "4. **X√°c ƒë·ªãnh** c√°c h√†ng c√≥ nhi·ªÅu gi√° tr·ªã thi·∫øu\n",
    "\n",
    "### üìã Y√™u c·∫ßu c·ª• th·ªÉ:\n",
    "- T·∫°o DataFrame v·ªõi c√°c c·ªôt: `TenNV`, `Tuoi`, `Luong`, `PhongBan`, `KinhNghiem`\n",
    "- S·ª≠ d·ª•ng `isna()`, `isnull()`, `notna()` ƒë·ªÉ ph√°t hi·ªán missing values\n",
    "- ƒê·∫øm s·ªë l∆∞·ª£ng missing theo t·ª´ng c·ªôt v√† t·ªïng s·ªë\n",
    "- T√¨m c√°c h√†ng c√≥ √≠t nh·∫•t 2 gi√° tr·ªã thi·∫øu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58485a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35121738",
   "metadata": {},
   "source": [
    "# üìä B√ÄI T·∫¨P 1.1: Ph√°t hi·ªán d·ªØ li·ªáu thi·∫øu\n",
    "# H√£y ho√†n th√†nh c√°c y√™u c·∫ßu d∆∞·ªõi ƒë√¢y:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. T·∫°o DataFrame v·ªõi d·ªØ li·ªáu nh√¢n vi√™n c√≥ missing values\n",
    "print(\"üè¢ T·∫°o DataFrame nh√¢n vi√™n v·ªõi missing values...\")\n",
    "\n",
    "# TODO: T·∫°o DataFrame v·ªõi c√°c c·ªôt: TenNV, Tuoi, Luong, PhongBan, KinhNghiem\n",
    "# G·ª£i √Ω: S·ª≠ d·ª•ng None ho·∫∑c np.nan cho missing values\n",
    "data_nhanvien = {\n",
    "    'TenNV': ['Nguy·ªÖn VƒÉn A', 'Tr·∫ßn Th·ªã B', 'L√™ VƒÉn C', 'Ph·∫°m Th·ªã D', 'Ho√†ng VƒÉn E'],\n",
    "    'Tuoi': [25, None, 30, 28, None],  # TODO: Th√™m missing values\n",
    "    'Luong': [15000000, 18000000, None, 22000000, None],  # TODO: Th√™m missing values\n",
    "    'PhongBan': ['IT', None, 'Marketing', 'IT', 'HR'],  # TODO: Th√™m missing values\n",
    "    'KinhNghiem': [2, 5, None, 7, 1]  # TODO: Th√™m missing values\n",
    "}\n",
    "\n",
    "df_nhanvien = pd.DataFrame(data_nhanvien)\n",
    "print(\"üìã DataFrame nh√¢n vi√™n:\")\n",
    "print(df_nhanvien)\n",
    "\n",
    "# 2. Ph√°t hi·ªán d·ªØ li·ªáu thi·∫øu\n",
    "print(\"\\nüîç Ph√°t hi·ªán d·ªØ li·ªáu thi·∫øu...\")\n",
    "\n",
    "# TODO: S·ª≠ d·ª•ng isna() ƒë·ªÉ ki·ªÉm tra missing values\n",
    "print(\"Ki·ªÉm tra missing values v·ªõi isna():\")\n",
    "# print(df_nhanvien.isna())\n",
    "\n",
    "# TODO: S·ª≠ d·ª•ng notna() ƒë·ªÉ ki·ªÉm tra d·ªØ li·ªáu c√≥ s·∫µn\n",
    "print(\"\\nKi·ªÉm tra d·ªØ li·ªáu c√≥ s·∫µn v·ªõi notna():\")\n",
    "# print(df_nhanvien.notna())\n",
    "\n",
    "# 3. ƒê·∫øm s·ªë l∆∞·ª£ng d·ªØ li·ªáu thi·∫øu\n",
    "print(\"\\nüìä ƒê·∫øm s·ªë l∆∞·ª£ng missing values...\")\n",
    "\n",
    "# TODO: ƒê·∫øm missing values theo t·ª´ng c·ªôt\n",
    "print(\"S·ªë l∆∞·ª£ng missing theo c·ªôt:\")\n",
    "# missing_count = df_nhanvien.isna().sum()\n",
    "# print(missing_count)\n",
    "\n",
    "# TODO: T√≠nh t·ª∑ l·ªá missing (%)\n",
    "print(\"\\nT·ª∑ l·ªá missing (%):\")\n",
    "# missing_percent = (missing_count / len(df_nhanvien)) * 100\n",
    "# print(missing_percent.round(2))\n",
    "\n",
    "# TODO: T·ªïng s·ªë missing values\n",
    "print(\"\\nT·ªïng s·ªë missing values:\")\n",
    "# total_missing = df_nhanvien.isna().sum().sum()\n",
    "# print(f\"T·ªïng: {total_missing}\")\n",
    "\n",
    "# 4. T√¨m c√°c h√†ng c√≥ √≠t nh·∫•t 2 gi√° tr·ªã thi·∫øu\n",
    "print(\"\\nüéØ T√¨m h√†ng c√≥ √≠t nh·∫•t 2 missing values...\")\n",
    "\n",
    "# TODO: T√¨m h√†ng c√≥ >= 2 missing values\n",
    "# G·ª£i √Ω: S·ª≠ d·ª•ng df.isna().sum(axis=1) >= 2\n",
    "# rows_with_multiple_missing = df_nhanvien[df_nhanvien.isna().sum(axis=1) >= 2]\n",
    "# print(\"C√°c h√†ng c√≥ √≠t nh·∫•t 2 missing values:\")\n",
    "# print(rows_with_multiple_missing)\n",
    "\n",
    "print(\"\\n‚úÖ Ho√†n th√†nh B√†i t·∫≠p 1.1!\")\n",
    "print(\"üí° H√£y b·ªè comment (#) v√† ch·∫°y c√°c d√≤ng code ƒë·ªÉ xem k·∫øt qu·∫£!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. T·∫°o DataFrame v·ªõi d·ªØ li·ªáu thi·∫øu\n",
    "data_nhanvien = {\n",
    "    'TenNV': ['Nguy·ªÖn VƒÉn A', 'Tr·∫ßn Th·ªã B', 'L√™ VƒÉn C', 'Ph·∫°m Th·ªã D', 'Ho√†ng VƒÉn E'],\n",
    "    'Tuoi': [25, None, 30, 28, None],\n",
    "    'Luong': [15000000, 18000000, None, 22000000, None],\n",
    "    'PhongBan': ['IT', None, 'Marketing', 'IT', 'HR'],\n",
    "    'KinhNghiem': [2, 5, None, 7, 1]\n",
    "}\n",
    "\n",
    "df_nhanvien = pd.DataFrame(data_nhanvien)\n",
    "print(\"DataFrame nh√¢n vi√™n v·ªõi d·ªØ li·ªáu thi·∫øu:\")\n",
    "print(df_nhanvien)\n",
    "\n",
    "# 2. Ph√°t hi·ªán d·ªØ li·ªáu thi·∫øu\n",
    "\n",
    "# 3. ƒê·∫øm s·ªë l∆∞·ª£ng d·ªØ li·ªáu thi·∫øu\n",
    "\n",
    "# 4. T√¨m c√°c h√†ng c√≥ √≠t nh·∫•t 2 gi√° tr·ªã thi·∫øu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba0cc45",
   "metadata": {},
   "source": [
    "## B√†i 1.2 - X·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu b·∫±ng dropna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60280879",
   "metadata": {},
   "source": [
    "1. S·ª≠ d·ª•ng DataFrame t·ª´ b√†i 1.1, √°p d·ª•ng ph∆∞∆°ng th·ª©c `dropna()` v·ªõi c√°c tham s·ªë kh√°c nhau:\n",
    "   - Lo·∫°i b·ªè t·∫•t c·∫£ h√†ng c√≥ √≠t nh·∫•t 1 gi√° tr·ªã thi·∫øu (`how='any'`)\n",
    "   - Lo·∫°i b·ªè h√†ng ch·ªâ khi t·∫•t c·∫£ gi√° tr·ªã ƒë·ªÅu thi·∫øu (`how='all'`)\n",
    "   - Lo·∫°i b·ªè c·ªôt c√≥ d·ªØ li·ªáu thi·∫øu (`axis=1`)\n",
    "\n",
    "2. √Åp d·ª•ng `dropna()` ch·ªâ tr√™n m·ªôt s·ªë c·ªôt c·ª• th·ªÉ (`subset`).\n",
    "\n",
    "3. So s√°nh k√≠ch th∆∞·ªõc DataFrame tr∆∞·ªõc v√† sau khi √°p d·ª•ng t·ª´ng ph∆∞∆°ng ph√°p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. T·∫°o DataFrame v·ªõi d·ªØ li·ªáu thi·∫øu\n",
    "data_nhanvien = {\n",
    "    'TenNV': ['Nguy·ªÖn VƒÉn A', 'Tr·∫ßn Th·ªã B', 'L√™ VƒÉn C', 'Ph·∫°m Th·ªã D', 'Ho√†ng VƒÉn E'],\n",
    "    'Tuoi': [25, None, 30, 28, None],\n",
    "    'Luong': [15000000, 18000000, None, 22000000, None],\n",
    "    'PhongBan': ['IT', None, 'Marketing', 'IT', 'HR'],\n",
    "    'KinhNghiem': [2, 5, None, 7, 1]\n",
    "}\n",
    "\n",
    "df_nhanvien = pd.DataFrame(data_nhanvien)\n",
    "\n",
    "# S·ª≠ d·ª•ng DataFrame t·ª´ b√†i 1.1\n",
    "print(\"DataFrame g·ªëc:\")\n",
    "print(df_nhanvien)\n",
    "print(f\"K√≠ch th∆∞·ªõc g·ªëc: {df_nhanvien.shape}\")\n",
    "\n",
    "# 1. Lo·∫°i b·ªè h√†ng c√≥ √≠t nh·∫•t 1 gi√° tr·ªã thi·∫øu\n",
    "\n",
    "# 2. Lo·∫°i b·ªè h√†ng ch·ªâ khi t·∫•t c·∫£ gi√° tr·ªã ƒë·ªÅu thi·∫øu\n",
    "\n",
    "# 3. Lo·∫°i b·ªè c·ªôt c√≥ d·ªØ li·ªáu thi·∫øu\n",
    "\n",
    "# 4. Lo·∫°i b·ªè d·ª±a tr√™n subset c·ªßa c√°c c·ªôt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2eec8c",
   "metadata": {},
   "source": [
    "## B√†i 1.3 - Thay th·∫ø d·ªØ li·ªáu thi·∫øu b·∫±ng fillna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636b67a4",
   "metadata": {},
   "source": [
    "1. S·ª≠ d·ª•ng DataFrame t·ª´ b√†i 1.1, thay th·∫ø d·ªØ li·ªáu thi·∫øu b·∫±ng c√°c ph∆∞∆°ng ph√°p kh√°c nhau:\n",
    "   - Thay th·∫ø b·∫±ng gi√° tr·ªã c·ªë ƒë·ªãnh (0 cho s·ªë, \"Kh√¥ng x√°c ƒë·ªãnh\" cho chu·ªói)\n",
    "   - Thay th·∫ø b·∫±ng gi√° tr·ªã trung b√¨nh (mean) cho c√°c c·ªôt s·ªë\n",
    "   - Thay th·∫ø b·∫±ng gi√° tr·ªã trung v·ªã (median) cho c√°c c·ªôt s·ªë\n",
    "   - Thay th·∫ø b·∫±ng gi√° tr·ªã ph·ªï bi·∫øn nh·∫•t (mode) cho c√°c c·ªôt ph√¢n lo·∫°i\n",
    "\n",
    "2. T·∫°o m·ªôt DataFrame kh√°c c√≥ d·ªØ li·ªáu chu·ªói th·ªùi gian v√† √°p d·ª•ng forward fill v√† backward fill.\n",
    "\n",
    "3. So s√°nh k·∫øt qu·∫£ c·ªßa c√°c ph∆∞∆°ng ph√°p thay th·∫ø kh√°c nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. T·∫°o DataFrame v·ªõi d·ªØ li·ªáu thi·∫øu\n",
    "data_nhanvien = {\n",
    "    'TenNV': ['Nguy·ªÖn VƒÉn A', 'Tr·∫ßn Th·ªã B', 'L√™ VƒÉn C', 'Ph·∫°m Th·ªã D', 'Ho√†ng VƒÉn E'],\n",
    "    'Tuoi': [25, None, 30, 28, None],\n",
    "    'Luong': [15000000, 18000000, None, 22000000, None],\n",
    "    'PhongBan': ['IT', None, 'Marketing', 'IT', 'HR'],\n",
    "    'KinhNghiem': [2, 5, None, 7, 1]\n",
    "}\n",
    "\n",
    "df_nhanvien = pd.DataFrame(data_nhanvien)\n",
    "\n",
    "# 1. Thay th·∫ø b·∫±ng c√°c ph∆∞∆°ng ph√°p kh√°c nhau\n",
    "print(\"DataFrame g·ªëc:\")\n",
    "print(df_nhanvien)\n",
    "\n",
    "# Thay th·∫ø b·∫±ng gi√° tr·ªã c·ªë ƒë·ªãnh\n",
    "\n",
    "# Thay th·∫ø b·∫±ng mean cho c·ªôt s·ªë\n",
    "\n",
    "# Thay th·∫ø b·∫±ng median cho c·ªôt s·ªë\n",
    "\n",
    "# Thay th·∫ø b·∫±ng mode cho c·ªôt ph√¢n lo·∫°i\n",
    "\n",
    "# 2. T·∫°o DataFrame chu·ªói th·ªùi gian ƒë·ªÉ demo forward/backward fill\n",
    "data_timeseries = {\n",
    "    'ngay': pd.date_range('2024-01-01', periods=7, freq='D'),\n",
    "    'gia_co_phieu': [100, None, 105, None, None, 110, 115],\n",
    "    'khoi_luong': [1000, 1200, None, 1500, None, None, 1800]\n",
    "}\n",
    "\n",
    "df_timeseries = pd.DataFrame(data_timeseries)\n",
    "print(\"\\nDataFrame chu·ªói th·ªùi gian:\")\n",
    "print(df_timeseries)\n",
    "\n",
    "# Forward fill v√† backward fill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0aea84",
   "metadata": {},
   "source": [
    "# Ph·∫ßn 2: X·ª≠ l√Ω d·ªØ li·ªáu tr√πng l·∫∑p (Duplicate Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca43bab",
   "metadata": {},
   "source": [
    "## B√†i 2.1 - Ph√°t hi·ªán v√† x·ª≠ l√Ω d·ªØ li·ªáu tr√πng l·∫∑p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f6f630",
   "metadata": {},
   "source": [
    "1. T·∫°o m·ªôt DataFrame v·ªÅ th√¥ng tin kh√°ch h√†ng c√≥ ch·ª©a d·ªØ li·ªáu tr√πng l·∫∑p v·ªõi c√°c c·ªôt: `TenKH`, `Tuoi`, `ThanhPho`, `SoDienThoai`.\n",
    "\n",
    "2. S·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c `duplicated()` ƒë·ªÉ ph√°t hi·ªán c√°c h√†ng tr√πng l·∫∑p.\n",
    "\n",
    "3. ƒê·∫øm t·ªïng s·ªë h√†ng tr√πng l·∫∑p v√† hi·ªÉn th·ªã c√°c h√†ng b·ªã tr√πng l·∫∑p.\n",
    "\n",
    "4. S·ª≠ d·ª•ng `drop_duplicates()` ƒë·ªÉ lo·∫°i b·ªè d·ªØ li·ªáu tr√πng l·∫∑p v·ªõi c√°c tham s·ªë kh√°c nhau:\n",
    "   - Gi·ªØ l·∫°i b·∫£n ghi ƒë·∫ßu ti√™n (`keep='first'`)\n",
    "   - Gi·ªØ l·∫°i b·∫£n ghi cu·ªëi c√πng (`keep='last'`)\n",
    "   - Lo·∫°i b·ªè t·∫•t c·∫£ b·∫£n ghi tr√πng l·∫∑p (`keep=False`)\n",
    "\n",
    "5. X·ª≠ l√Ω tr√πng l·∫∑p d·ª±a tr√™n m·ªôt s·ªë c·ªôt c·ª• th·ªÉ (`subset`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fefe7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. T·∫°o DataFrame c√≥ d·ªØ li·ªáu tr√πng l·∫∑p\n",
    "data_khachhang = {\n",
    "    'TenKH': ['Nguy·ªÖn VƒÉn A', 'Tr·∫ßn Th·ªã B', 'Nguy·ªÖn VƒÉn A', 'L√™ VƒÉn C', 'Tr·∫ßn Th·ªã B', 'Ph·∫°m Th·ªã D'],\n",
    "    'Tuoi': [25, 30, 25, 35, 30, 28],\n",
    "    'ThanhPho': ['H√† N·ªôi', 'TP.HCM', 'H√† N·ªôi', 'ƒê√† N·∫µng', 'TP.HCM', 'H√† N·ªôi'],\n",
    "    'SoDienThoai': ['0123456789', '0987654321', '0123456789', '0111222333', '0987654321', '0999888777']\n",
    "}\n",
    "\n",
    "df_khachhang = pd.DataFrame(data_khachhang)\n",
    "print(\"DataFrame kh√°ch h√†ng c√≥ d·ªØ li·ªáu tr√πng l·∫∑p:\")\n",
    "print(df_khachhang)\n",
    "\n",
    "# 2. Ph√°t hi·ªán d·ªØ li·ªáu tr√πng l·∫∑p\n",
    "\n",
    "# 3. ƒê·∫øm v√† hi·ªÉn th·ªã c√°c h√†ng tr√πng l·∫∑p\n",
    "\n",
    "# 4. Lo·∫°i b·ªè d·ªØ li·ªáu tr√πng l·∫∑p v·ªõi c√°c tham s·ªë kh√°c nhau\n",
    "\n",
    "# 5. X·ª≠ l√Ω tr√πng l·∫∑p d·ª±a tr√™n subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c323cc",
   "metadata": {},
   "source": [
    "# Ph·∫ßn 3: Bi·∫øn ƒë·ªïi v√† chu·∫©n h√≥a d·ªØ li·ªáu (Data Transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0afff8",
   "metadata": {},
   "source": [
    "## B√†i 3.1 - Min-Max Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c468fe",
   "metadata": {},
   "source": [
    "1. T·∫°o m·ªôt DataFrame v·ªÅ th√¥ng tin b·∫•t ƒë·ªông s·∫£n v·ªõi c√°c c·ªôt: `DienTich` (m¬≤), `Gia` (tri·ªáu VND), `SoPhong`, `TuoiNha` (nƒÉm).\n",
    "\n",
    "2. √Åp d·ª•ng Min-Max Normalization b·∫±ng c√¥ng th·ª©c th·ªß c√¥ng cho c·ªôt `Gia`.\n",
    "\n",
    "3. S·ª≠ d·ª•ng `MinMaxScaler` t·ª´ scikit-learn ƒë·ªÉ chu·∫©n h√≥a t·∫•t c·∫£ c√°c c·ªôt s·ªë.\n",
    "\n",
    "4. So s√°nh k·∫øt qu·∫£ tr∆∞·ªõc v√† sau khi chu·∫©n h√≥a b·∫±ng c√°ch v·∫Ω bi·ªÉu ƒë·ªì ho·∫∑c th·ªëng k√™ m√¥ t·∫£."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c887da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. T·∫°o DataFrame v·ªÅ b·∫•t ƒë·ªông s·∫£n\n",
    "data_batdongsan = {\n",
    "    'DienTich': [50, 80, 120, 200, 300],\n",
    "    'Gia': [2000, 3500, 5000, 8000, 12000],  # tri·ªáu VND\n",
    "    'SoPhong': [2, 3, 4, 5, 6],\n",
    "    'TuoiNha': [5, 10, 15, 20, 25]  # nƒÉm\n",
    "}\n",
    "\n",
    "df_batdongsan = pd.DataFrame(data_batdongsan)\n",
    "print(\"DataFrame b·∫•t ƒë·ªông s·∫£n g·ªëc:\")\n",
    "print(df_batdongsan)\n",
    "print(\"\\nTh·ªëng k√™ m√¥ t·∫£:\")\n",
    "print(df_batdongsan.describe())\n",
    "\n",
    "# 2. Min-Max Normalization th·ªß c√¥ng cho c·ªôt Gia\n",
    "\n",
    "# 3. S·ª≠ d·ª•ng MinMaxScaler cho t·∫•t c·∫£ c·ªôt s·ªë\n",
    "\n",
    "# 4. So s√°nh k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31bb225",
   "metadata": {},
   "source": [
    "## B√†i 3.2 - Z-score Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802714f",
   "metadata": {},
   "source": [
    "1. S·ª≠ d·ª•ng DataFrame t·ª´ b√†i 3.1, √°p d·ª•ng Z-score Standardization b·∫±ng c√¥ng th·ª©c th·ªß c√¥ng cho c·ªôt `DienTich`.\n",
    "\n",
    "2. S·ª≠ d·ª•ng `StandardScaler` t·ª´ scikit-learn ƒë·ªÉ chu·∫©n h√≥a t·∫•t c·∫£ c√°c c·ªôt s·ªë.\n",
    "\n",
    "3. Ki·ªÉm tra xem d·ªØ li·ªáu sau standardization c√≥ mean ‚âà 0 v√† std ‚âà 1 kh√¥ng.\n",
    "\n",
    "4. So s√°nh k·∫øt qu·∫£ gi·ªØa Min-Max Normalization v√† Z-score Standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. T·∫°o DataFrame v·ªÅ b·∫•t ƒë·ªông s·∫£n\n",
    "data_batdongsan = {\n",
    "    'DienTich': [50, 80, 120, 200, 300],\n",
    "    'Gia': [2000, 3500, 5000, 8000, 12000],  # tri·ªáu VND\n",
    "    'SoPhong': [2, 3, 4, 5, 6],\n",
    "    'TuoiNha': [5, 10, 15, 20, 25]  # nƒÉm\n",
    "}\n",
    "\n",
    "df_batdongsan = pd.DataFrame(data_batdongsan)\n",
    "# 1. Z-score Standardization th·ªß c√¥ng cho c·ªôt DienTich\n",
    "print(\"DataFrame g·ªëc:\")\n",
    "print(df_batdongsan)\n",
    "\n",
    "# T√≠nh Z-score th·ªß c√¥ng cho DienTich\n",
    "\n",
    "# 2. S·ª≠ d·ª•ng StandardScaler cho t·∫•t c·∫£ c·ªôt\n",
    "\n",
    "# 3. Ki·ªÉm tra mean v√† std\n",
    "\n",
    "# 4. So s√°nh Min-Max vs Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f4168",
   "metadata": {},
   "source": [
    "## B√†i 3.3 - Robust Scaler v√† x·ª≠ l√Ω Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de9923",
   "metadata": {},
   "source": [
    "1. T·∫°o m·ªôt DataFrame m·ªõi c√≥ ch·ª©a outliers (gi√° tr·ªã ngo·∫°i lai) trong c·ªôt `Gia`.\n",
    "\n",
    "2. So s√°nh hi·ªáu qu·∫£ c·ªßa 3 ph∆∞∆°ng ph√°p scaling khi c√≥ outliers:\n",
    "   - MinMaxScaler\n",
    "   - StandardScaler  \n",
    "   - RobustScaler\n",
    "\n",
    "3. S·ª≠ d·ª•ng `RobustScaler` ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu c√≥ outliers.\n",
    "\n",
    "4. V·∫Ω bi·ªÉu ƒë·ªì ho·∫∑c th·ªëng k√™ ƒë·ªÉ th·∫•y r√µ s·ª± kh√°c bi·ªát gi·ªØa c√°c ph∆∞∆°ng ph√°p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6d2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import pandas as pd\n",
    "\n",
    "# 1. T·∫°o DataFrame c√≥ outliers\n",
    "data_outliers = {\n",
    "    'DienTich': [50, 80, 120, 200, 300, 95],\n",
    "    'Gia': [2000, 3500, 5000, 8000, 50000, 4200],  # 50000 l√† outlier\n",
    "    'SoPhong': [2, 3, 4, 5, 8, 3],  # 8 l√† outlier\n",
    "    'TuoiNha': [5, 10, 15, 20, 25, 12]\n",
    "}\n",
    "\n",
    "df_outliers = pd.DataFrame(data_outliers)\n",
    "print(\"DataFrame c√≥ outliers:\")\n",
    "print(df_outliers)\n",
    "print(\"\\nTh·ªëng k√™ m√¥ t·∫£:\")\n",
    "print(df_outliers.describe())\n",
    "\n",
    "# 2. So s√°nh 3 ph∆∞∆°ng ph√°p scaling\n",
    "\n",
    "# 3. S·ª≠ d·ª•ng RobustScaler\n",
    "\n",
    "# 4. Ph√¢n t√≠ch k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579ee60",
   "metadata": {},
   "source": [
    "# Ph·∫ßn 4: X·ª≠ l√Ω chu·ªói k√Ω t·ª± (String Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f0fdc",
   "metadata": {},
   "source": [
    "## B√†i 4.1 - X·ª≠ l√Ω chu·ªói k√Ω t·ª± c∆° b·∫£n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feedbde",
   "metadata": {},
   "source": [
    "1. T·∫°o m·ªôt DataFrame v·ªÅ th√¥ng tin s·∫£n ph·∫©m v·ªõi c√°c c·ªôt: `TenSP`, `MoTa`, `DanhMuc` c√≥ d·ªØ li·ªáu chu·ªói kh√¥ng ƒë·ªìng nh·∫•t (c√≥ kho·∫£ng tr·∫Øng th·ª´a, ch·ªØ hoa/th∆∞·ªùng kh√°c nhau).\n",
    "\n",
    "2. S·ª≠ d·ª•ng c√°c ph∆∞∆°ng th·ª©c x·ª≠ l√Ω chu·ªói c∆° b·∫£n:\n",
    "   - `.str.lower()`, `.str.upper()`, `.str.title()`\n",
    "   - `.str.strip()` ƒë·ªÉ lo·∫°i b·ªè kho·∫£ng tr·∫Øng\n",
    "   - `.str.replace()` ƒë·ªÉ thay th·∫ø k√Ω t·ª±\n",
    "   - `.str.len()` ƒë·ªÉ t√≠nh ƒë·ªô d√†i chu·ªói\n",
    "\n",
    "3. T√¨m ki·∫øm v√† l·ªçc d·ªØ li·ªáu b·∫±ng:\n",
    "   - `.str.contains()` ƒë·ªÉ t√¨m s·∫£n ph·∫©m ch·ª©a t·ª´ kh√≥a\n",
    "   - `.str.startswith()` v√† `.str.endswith()`\n",
    "\n",
    "4. T√°ch chu·ªói b·∫±ng `.str.split()` ƒë·ªÉ t√°ch danh m·ª•c s·∫£n ph·∫©m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. T·∫°o DataFrame v·ªõi d·ªØ li·ªáu chu·ªói kh√¥ng ƒë·ªìng nh·∫•t\n",
    "data_sanpham = {\n",
    "    'TenSP': ['  iPhone 15 Pro  ', 'samsung galaxy s23', 'XIAOMI Mi 13', '  MacBook Air M2  ', 'dell XPS 13'],\n",
    "    'MoTa': ['ƒêi·ªán tho·∫°i th√¥ng minh cao c·∫•p', '  Smartphone Android m·ªõi nh·∫•t  ', 'ƒêI·ªÜN THO·∫†I XIAOMI GI√Å R·∫∫', 'Laptop Apple si√™u m·ªèng', '  m√°y t√≠nh x√°ch tay Dell  '],\n",
    "    'DanhMuc': ['ƒêi·ªán tho·∫°i/Apple', 'ƒêi·ªán tho·∫°i/Samsung', 'ƒêi·ªán tho·∫°i/Xiaomi', 'Laptop/Apple', 'Laptop/Dell']\n",
    "}\n",
    "\n",
    "df_sanpham = pd.DataFrame(data_sanpham)\n",
    "print(\"DataFrame s·∫£n ph·∫©m v·ªõi d·ªØ li·ªáu chu·ªói kh√¥ng ƒë·ªìng nh·∫•t:\")\n",
    "print(df_sanpham)\n",
    "\n",
    "# 2. X·ª≠ l√Ω chu·ªói c∆° b·∫£n\n",
    "\n",
    "# 3. T√¨m ki·∫øm v√† l·ªçc d·ªØ li·ªáu\n",
    "\n",
    "# 4. T√°ch chu·ªói"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b145862",
   "metadata": {},
   "source": [
    "## B√†i 4.2 - Regular Expressions (Regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423f1a65",
   "metadata": {},
   "source": [
    "1. T·∫°o m·ªôt DataFrame ch·ª©a th√¥ng tin li√™n h·ªá kh√°ch h√†ng v·ªõi c·ªôt `ThongTin` ch·ª©a ƒë·ªãa ch·ªâ email, s·ªë ƒëi·ªán tho·∫°i, website.\n",
    "\n",
    "2. S·ª≠ d·ª•ng regex ƒë·ªÉ tr√≠ch xu·∫•t:\n",
    "   - ƒê·ªãa ch·ªâ email b·∫±ng pattern cho email\n",
    "   - S·ªë ƒëi·ªán tho·∫°i Vi·ªát Nam (c√°c ƒë·ªãnh d·∫°ng kh√°c nhau)\n",
    "   - Website/URL\n",
    "\n",
    "3. L√†m s·∫°ch v√† chu·∫©n h√≥a s·ªë ƒëi·ªán tho·∫°i v·ªÅ ƒë·ªãnh d·∫°ng th·ªëng nh·∫•t.\n",
    "\n",
    "4. T·∫°o c√°c c·ªôt ri√™ng bi·ªát cho email, s·ªë ƒëi·ªán tho·∫°i, website ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 1. T·∫°o DataFrame v·ªõi th√¥ng tin li√™n h·ªá\n",
    "data_lienhe = {\n",
    "    'KhachHang': ['C√¥ng ty A', 'Kh√°ch h√†ng B', 'Doanh nghi·ªáp C', 'C√° nh√¢n D', 'T·ªï ch·ª©c E'],\n",
    "    'ThongTin': [\n",
    "        'Email: info@congtyA.com, SƒêT: 0123-456-789, Website: https://congtyA.com',\n",
    "        'Li√™n h·ªá: khachhangB@gmail.com ho·∫∑c g·ªçi +84 987 654 321',\n",
    "        'Hotline: (028) 3825-7863, web: www.doanhnghiepC.vn, mail: contact@doanhnghiepC.vn',\n",
    "        'Phone: 0987.654.321, email: canhanD@yahoo.com',\n",
    "        'Tel: 1900-1234, site: https://tochucE.org.vn, email: admin@tochucE.org.vn'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_lienhe = pd.DataFrame(data_lienhe)\n",
    "print(\"DataFrame th√¥ng tin li√™n h·ªá:\")\n",
    "print(df_lienhe)\n",
    "\n",
    "# 2. Tr√≠ch xu·∫•t th√¥ng tin b·∫±ng regex\n",
    "\n",
    "# 3. Chu·∫©n h√≥a s·ªë ƒëi·ªán tho·∫°i\n",
    "\n",
    "# 4. T·∫°o c√°c c·ªôt ri√™ng bi·ªát"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f8a2a",
   "metadata": {},
   "source": [
    "# Ph·∫ßn 5: X·ª≠ l√Ω d·ªØ li·ªáu ph√¢n lo·∫°i (Categorical Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c44f937",
   "metadata": {},
   "source": [
    "## B√†i 5.1 - Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb93869",
   "metadata": {},
   "source": [
    "1. T·∫°o m·ªôt DataFrame v·ªÅ ƒë√°nh gi√° kh√≥a h·ªçc v·ªõi c√°c c·ªôt: `TenKhoaHoc`, `TrinhDo` (C∆° b·∫£n, Trung c·∫•p, N√¢ng cao), `XepLoai` (K√©m, Trung b√¨nh, Kh√°, Gi·ªèi, Xu·∫•t s·∫Øc), `NgonNgu`.\n",
    "\n",
    "2. √Åp d·ª•ng Label Encoding cho:\n",
    "   - C·ªôt `TrinhDo` (c√≥ th·ª© t·ª± t·ª± nhi√™n)\n",
    "   - C·ªôt `XepLoai` (c√≥ th·ª© t·ª± t·ª± nhi√™n)\n",
    "   - C·ªôt `NgonNgu` (kh√¥ng c√≥ th·ª© t·ª± t·ª± nhi√™n)\n",
    "\n",
    "3. So s√°nh k·∫øt qu·∫£ Label Encoding v·ªõi pandas Category v√† th·ª© t·ª± t√πy ch·ªânh cho d·ªØ li·ªáu ordinal.\n",
    "\n",
    "4. Ph√¢n t√≠ch ∆∞u/nh∆∞·ª£c ƒëi·ªÉm c·ªßa Label Encoding cho t·ª´ng lo·∫°i d·ªØ li·ªáu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas \n",
    "\n",
    "# 1. T·∫°o DataFrame v·ªÅ ƒë√°nh gi√° kh√≥a h·ªçc\n",
    "data_khoahoc = {\n",
    "    'TenKhoaHoc': ['Python c∆° b·∫£n', 'Java n√¢ng cao', 'SQL trung c·∫•p', 'HTML c∆° b·∫£n', 'React n√¢ng cao'],\n",
    "    'TrinhDo': ['C∆° b·∫£n', 'N√¢ng cao', 'Trung c·∫•p', 'C∆° b·∫£n', 'N√¢ng cao'],\n",
    "    'XepLoai': ['Kh√°', 'Xu·∫•t s·∫Øc', 'Gi·ªèi', 'Trung b√¨nh', 'Gi·ªèi'],\n",
    "    'NgonNgu': ['Python', 'Java', 'SQL', 'HTML', 'JavaScript']\n",
    "}\n",
    "\n",
    "df_khoahoc = pd.DataFrame(data_khoahoc)\n",
    "print(\"DataFrame ƒë√°nh gi√° kh√≥a h·ªçc:\")\n",
    "print(df_khoahoc)\n",
    "\n",
    "# 2. √Åp d·ª•ng Label Encoding\n",
    "\n",
    "# 3. S·ª≠ d·ª•ng pandas Category v·ªõi th·ª© t·ª± t√πy ch·ªânh\n",
    "\n",
    "# 4. Ph√¢n t√≠ch k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a79a5b",
   "metadata": {},
   "source": [
    "## B√†i 5.2 - One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9067ebbc",
   "metadata": {},
   "source": [
    "1. S·ª≠ d·ª•ng DataFrame t·ª´ b√†i 5.1, √°p d·ª•ng One-Hot Encoding cho c√°c c·ªôt ph√¢n lo·∫°i nominal (kh√¥ng c√≥ th·ª© t·ª± t·ª± nhi√™n).\n",
    "\n",
    "2. So s√°nh hai ph∆∞∆°ng ph√°p One-Hot Encoding:\n",
    "   - S·ª≠ d·ª•ng `pd.get_dummies()`\n",
    "   - S·ª≠ d·ª•ng `OneHotEncoder` t·ª´ scikit-learn\n",
    "\n",
    "3. X·ª≠ l√Ω tham s·ªë `drop_first=True` ƒë·ªÉ tr√°nh multicollinearity.\n",
    "\n",
    "4. So s√°nh k√≠ch th∆∞·ªõc DataFrame tr∆∞·ªõc v√† sau One-Hot Encoding v√† ph√¢n t√≠ch ∆∞u/nh∆∞·ª£c ƒëi·ªÉm.\n",
    "\n",
    "5. K·∫øt h·ª£p c·∫£ Label Encoding v√† One-Hot Encoding trong c√πng m·ªôt DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58bb5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas \n",
    "\n",
    "# 1. T·∫°o DataFrame v·ªÅ ƒë√°nh gi√° kh√≥a h·ªçc\n",
    "data_khoahoc = {\n",
    "    'TenKhoaHoc': ['Python c∆° b·∫£n', 'Java n√¢ng cao', 'SQL trung c·∫•p', 'HTML c∆° b·∫£n', 'React n√¢ng cao'],\n",
    "    'TrinhDo': ['C∆° b·∫£n', 'N√¢ng cao', 'Trung c·∫•p', 'C∆° b·∫£n', 'N√¢ng cao'],\n",
    "    'XepLoai': ['Kh√°', 'Xu·∫•t s·∫Øc', 'Gi·ªèi', 'Trung b√¨nh', 'Gi·ªèi'],\n",
    "    'NgonNgu': ['Python', 'Java', 'SQL', 'HTML', 'JavaScript']\n",
    "}\n",
    "\n",
    "df_khoahoc = pd.DataFrame(data_khoahoc)\n",
    "print(\"DataFrame ƒë√°nh gi√° kh√≥a h·ªçc:\")\n",
    "print(df_khoahoc)\n",
    "\n",
    "# 2. One-Hot Encoding b·∫±ng pd.get_dummies()\n",
    "\n",
    "# 3. One-Hot Encoding b·∫±ng sklearn OneHotEncoder\n",
    "\n",
    "# 4. So s√°nh k√≠ch th∆∞·ªõc v√† ph√¢n t√≠ch\n",
    "\n",
    "# 5. K·∫øt h·ª£p Label Encoding v√† One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b4a1a",
   "metadata": {},
   "source": [
    "# Ph·∫ßn 6: B√†i t·∫≠p t·ªïng h·ª£p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067b799",
   "metadata": {},
   "source": [
    "## B√†i 6.1 - D·ª± √°n l√†m s·∫°ch d·ªØ li·ªáu ho√†n ch·ªânh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b6eb8b",
   "metadata": {},
   "source": [
    "**M√¥ t·∫£:** B·∫°n ƒë∆∞·ª£c cung c·∫•p d·ªØ li·ªáu kh·∫£o s√°t kh√°ch h√†ng c·ªßa m·ªôt c·ª≠a h√†ng online c√≥ nhi·ªÅu v·∫•n ƒë·ªÅ v·ªÅ ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu. H√£y √°p d·ª•ng t·∫•t c·∫£ ki·∫øn th·ª©c ƒë√£ h·ªçc ƒë·ªÉ l√†m s·∫°ch v√† chu·∫©n b·ªã d·ªØ li·ªáu.\n",
    "\n",
    "**C√°c b∆∞·ªõc th·ª±c hi·ªán:**\n",
    "\n",
    "1. **T·∫°o d·ªØ li·ªáu m·∫´u** c√≥ ch·ª©a t·∫•t c·∫£ c√°c v·∫•n ƒë·ªÅ:\n",
    "   - D·ªØ li·ªáu thi·∫øu ·ªü nhi·ªÅu c·ªôt\n",
    "   - D·ªØ li·ªáu tr√πng l·∫∑p\n",
    "   - Chu·ªói k√Ω t·ª± kh√¥ng ƒë·ªìng nh·∫•t\n",
    "   - D·ªØ li·ªáu ph√¢n lo·∫°i c·∫ßn encoding\n",
    "   - D·ªØ li·ªáu s·ªë c√≥ outliers c·∫ßn scaling\n",
    "\n",
    "2. **Ph√¢n t√≠ch v√† b√°o c√°o** t√¨nh tr·∫°ng d·ªØ li·ªáu ban ƒë·∫ßu\n",
    "\n",
    "3. **√Åp d·ª•ng c√°c k·ªπ thu·∫≠t l√†m s·∫°ch** ƒë√£ h·ªçc theo th·ª© t·ª± ph√π h·ª£p\n",
    "\n",
    "4. **So s√°nh v√† ƒë√°nh gi√°** k·∫øt qu·∫£ tr∆∞·ªõc/sau khi x·ª≠ l√Ω\n",
    "\n",
    "5. **Xu·∫•t d·ªØ li·ªáu s·∫°ch** s·∫µn s√†ng cho ph√¢n t√≠ch/machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B∆∞·ªõc 1: T·∫°o d·ªØ li·ªáu m·∫´u c√≥ nhi·ªÅu v·∫•n ƒë·ªÅ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu kh·∫£o s√°t kh√°ch h√†ng c√≥ v·∫•n ƒë·ªÅ\n",
    "data_khaosat = {\n",
    "    'ho_ten': ['  Nguy·ªÖn VƒÉn A  ', 'TR·∫¶N TH·ªä B', 'nguy·ªÖn vƒÉn a', '  L√™ VƒÉn C  ', 'Ph·∫°m Th·ªã D', 'TR·∫¶N TH·ªä B', 'Ho√†ng VƒÉn E', None],\n",
    "    'tuoi': [25, None, 25, 35, None, 30, 28, 45],\n",
    "    'gioi_tinh': ['Nam', 'N·ªØ', 'nam', 'Nam', 'N·ªØ', 'N·ªØ', None, 'Nam'],\n",
    "    'thu_nhap': [15000000, 25000000, None, 35000000, 18000000, 25000000, 150000000, 22000000],  # 150M l√† outlier\n",
    "    'muc_do_hai_long': ['R·∫•t h√†i l√≤ng', 'H√†i l√≤ng', None, 'B√¨nh th∆∞·ªùng', 'Kh√¥ng h√†i l√≤ng', 'H√†i l√≤ng', 'R·∫•t h√†i l√≤ng', 'H√†i l√≤ng'],\n",
    "    'so_lan_mua_hang': [5, 3, 5, 12, None, 3, 20, 8],\n",
    "    'email': ['  nguyena@gmail.com  ', 'tranthib@YAHOO.COM', None, 'levanc@outlook.com', '', 'tranthib@yahoo.com', 'hoange@company.vn', 'invalid-email'],\n",
    "    'thanh_pho': ['H√† N·ªôi', 'tp.hcm', 'H√† N·ªôi', 'ƒê√† N·∫µng', 'TP.HCM', 'tp.hcm', None, 'H·∫£i Ph√≤ng']\n",
    "}\n",
    "\n",
    "df_raw = pd.DataFrame(data_khaosat)\n",
    "print(\"=== D·ªÆ LI·ªÜU KH·∫¢O S√ÅT KH√ÅCH H√ÄNG (RAW) ===\")\n",
    "print(df_raw)\n",
    "\n",
    "# B∆∞·ªõc 2: Ph√¢n t√≠ch t√¨nh tr·∫°ng d·ªØ li·ªáu ban ƒë·∫ßu\n",
    "print(\"\\n=== PH√ÇN T√çCH D·ªÆ LI·ªÜU BAN ƒê·∫¶U ===\")\n",
    "\n",
    "# B∆∞·ªõc 3: √Åp d·ª•ng c√°c k·ªπ thu·∫≠t l√†m s·∫°ch\n",
    "\n",
    "# B∆∞·ªõc 4: So s√°nh k·∫øt qu·∫£\n",
    "\n",
    "# B∆∞·ªõc 5: Xu·∫•t d·ªØ li·ªáu s·∫°ch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
