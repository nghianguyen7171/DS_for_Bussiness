---

# üìö T·ªîNG K·∫æT B√ÄI H·ªåC

## üéØ Ki·∫øn th·ª©c ƒë√£ h·ªçc

### ‚úÖ Ph·∫ßn 1: X·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu
- **Ph√°t hi·ªán**: `.isna()`, `.isnull()`, `.isna().sum()`
- **Lo·∫°i b·ªè**: `.dropna(how='any'/'all', axis=0/1)`
- **Thay th·∫ø**: `.fillna(value/method/dict)`
- **D·ª± ƒëo√°n**: Random Forest, KNN Imputer

### ‚úÖ Ph·∫ßn 2: X·ª≠ l√Ω d·ªØ li·ªáu tr√πng l·∫∑p
- **Ph√°t hi·ªán**: `.duplicated()`
- **Lo·∫°i b·ªè**: `.drop_duplicates()`

### ‚úÖ Ph·∫ßn 3: Chu·∫©n h√≥a d·ªØ li·ªáu
- **MinMaxScaler**: ƒê∆∞a v·ªÅ [0,1]
- **StandardScaler**: Mean=0, Std=1
- **RobustScaler**: Median=0, IQR=1

### ‚úÖ Ph·∫ßn 4: X·ª≠ l√Ω chu·ªói k√Ω t·ª±
- **String methods**: `.str.lower()`, `.str.upper()`, `.str.strip()`
- **Regular Expressions**: Pattern matching v·ªõi `.str.contains()`, `.str.extract()`

### ‚úÖ Ph·∫ßn 5: D·ªØ li·ªáu ph√¢n lo·∫°i
- **Label Encoding**: Chuy·ªÉn th√†nh s·ªë nguy√™n
- **One-Hot Encoding**: T·∫°o binary columns

---

## üèÜ B√ÄI T·∫¨P TH·ª∞C H√ÄNH

### üìù Ph·∫ßn A: B√†i t·∫≠p c∆° b·∫£n (D·ªÖ)

**B√†i 1:** Ph√°t hi·ªán v√† x·ª≠ l√Ω missing data
```python
# T·∫°o DataFrame v·ªõi missing data
import pandas as pd
import numpy as np

data = {
    'Ten': ['An', 'B√¨nh', 'Chi', 'D≈©ng'],
    'Tuoi': [25, None, 30, 28],
    'Luong': [15000000, 18000000, None, 22000000]
}
df = pd.DataFrame(data)

# TODO: 
# 1. Ki·ªÉm tra s·ªë l∆∞·ª£ng missing values v·ªõi df.isna().sum()
# 2. Thay th·∫ø missing values b·∫±ng median
# 3. Hi·ªÉn th·ªã k·∫øt qu·∫£
```

**B√†i 2:** Lo·∫°i b·ªè duplicate data
```python
# T·∫°o DataFrame c√≥ duplicates
data = {
    'Ten': ['An', 'B√¨nh', 'Chi', 'An'],
    'Tuoi': [25, 30, 35, 25],
    'Luong': [15000000, 18000000, 20000000, 15000000]
}
df = pd.DataFrame(data)

# TODO:
# 1. Ph√°t hi·ªán duplicates v·ªõi df.duplicated()
# 2. Lo·∫°i b·ªè duplicates v·ªõi df.drop_duplicates()
# 3. Hi·ªÉn th·ªã k·∫øt qu·∫£
```

---

### üìù Ph·∫ßn B: B√†i t·∫≠p trung b√¨nh

**B√†i 3:** Chu·∫©n h√≥a d·ªØ li·ªáu
```python
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# D·ªØ li·ªáu nh√¢n vi√™n v·ªõi c√°c thang ƒëo kh√°c nhau
data = {
    'Ten': ['An', 'B√¨nh', 'Chi', 'D≈©ng', 'Eva'],
    'Luong': [15000000, 25000000, 30000000, 18000000, 22000000],
    'Tuoi': [25, 35, 40, 28, 32],
    'KinhNghiem': [2, 8, 12, 3, 6]
}
df = pd.DataFrame(data)

# TODO:
# 1. √Åp d·ª•ng MinMaxScaler cho c√°c c·ªôt s·ªë
# 2. √Åp d·ª•ng StandardScaler cho c√°c c·ªôt s·ªë
# 3. So s√°nh k·∫øt qu·∫£ v√† nh·∫≠n x√©t
```

**B√†i 4:** X·ª≠ l√Ω chu·ªói k√Ω t·ª±
```python
# D·ªØ li·ªáu kh√°ch h√†ng l·ªôn x·ªôn
data = {
    'Ten': ['  NGUY·ªÑN VƒÇN A  ', 'tr·∫ßn th·ªã b', 'L√ä VƒÇN C'],
    'Email': ['a@gmail.com', 'b@yahoo.com', 'c@hotmail.com'],
    'SDT': ['0123-456-789', '0987654321', '+84-987-654-321']
}
df = pd.DataFrame(data)

# TODO:
# 1. Chu·∫©n h√≥a t√™n: .str.strip(), .str.title()
# 2. Tr√≠ch xu·∫•t domain t·ª´ email: .str.extract()
# 3. Chu·∫©n h√≥a s·ªë ƒëi·ªán tho·∫°i: lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát
```

---

### üìù Ph·∫ßn C: B√†i t·∫≠p n√¢ng cao (Kh√≥)

**B√†i 5:** X·ª≠ l√Ω d·ªØ li·ªáu ph√¢n lo·∫°i
```python
from sklearn.preprocessing import LabelEncoder

# D·ªØ li·ªáu s·∫£n ph·∫©m
data = {
    'TenSP': ['iPhone 14', 'Samsung Galaxy', 'iPhone 13', 'Xiaomi Redmi'],
    'Hang': ['Apple', 'Samsung', 'Apple', 'Xiaomi'],
    'Gia': [25000000, 20000000, 20000000, 8000000],
    'DanhGia': ['T·ªët', 'Xu·∫•t s·∫Øc', 'T·ªët', 'Trung b√¨nh']
}
df = pd.DataFrame(data)

# TODO:
# 1. Label Encoding cho c·ªôt 'Hang'
# 2. One-Hot Encoding cho c·ªôt 'DanhGia' v·ªõi pd.get_dummies()
# 3. Chu·∫©n h√≥a c·ªôt 'Gia' v·ªõi StandardScaler
# 4. T·∫°o DataFrame cu·ªëi c√πng s·∫µn s√†ng cho ML
```

**B√†i 6:** D·ª± √°n t·ªïng h·ª£p (Homework)
```python
# D·ªØ li·ªáu kh·∫£o s√°t kh√°ch h√†ng (c√≥ nhi·ªÅu v·∫•n ƒë·ªÅ)
data = {
    'ID': [1, 2, 3, 4, 5, 6, 7, 8],
    'Ten': ['An', 'B√¨nh', 'Chi', 'D≈©ng', 'Eva', 'An', 'B√¨nh', 'Giang'],
    'Tuoi': [25, None, 30, 28, None, 25, 35, 32],
    'Luong': [15000000, 18000000, None, 22000000, 16000000, 15000000, 20000000, 25000000],
    'ThanhPho': ['H√† N·ªôi', 'TP.HCM', 'H√† N·ªôi', 'ƒê√† N·∫µng', 'TP.HCM', 'H√† N·ªôi', 'TP.HCM', 'H√† N·ªôi'],
    'DanhGia': ['T·ªët', 'Xu·∫•t s·∫Øc', 'T·ªët', 'Trung b√¨nh', 'T·ªët', 'T·ªët', 'Xu·∫•t s·∫Øc', 'T·ªët']
}
df = pd.DataFrame(data)

# TODO: L√†m s·∫°ch d·ªØ li·ªáu ho√†n ch·ªânh
# B∆∞·ªõc 1: Ph√¢n t√≠ch d·ªØ li·ªáu
#   - Ki·ªÉm tra th√¥ng tin: df.info()
#   - Ki·ªÉm tra missing: df.isna().sum()
#   - Ki·ªÉm tra duplicates: df.duplicated().sum()

# B∆∞·ªõc 2: X·ª≠ l√Ω missing values
#   - Quy·∫øt ƒë·ªãnh ph∆∞∆°ng ph√°p: drop hay fill?
#   - V·ªõi Tuoi: fillna(median)
#   - V·ªõi Luong: fillna(mean)

# B∆∞·ªõc 3: X·ª≠ l√Ω duplicates
#   - Lo·∫°i b·ªè h√†ng tr√πng l·∫∑p ho√†n to√†n

# B∆∞·ªõc 4: Chu·∫©n h√≥a d·ªØ li·ªáu s·ªë
#   - √Åp d·ª•ng StandardScaler cho Tuoi, Luong

# B∆∞·ªõc 5: Encoding d·ªØ li·ªáu ph√¢n lo·∫°i
#   - One-Hot cho ThanhPho
#   - Label Encoding cho DanhGia

# B∆∞·ªõc 6: T·∫°o b√°o c√°o
#   - In k√≠ch th∆∞·ªõc tr∆∞·ªõc/sau
#   - In c√°c c·ªôt cu·ªëi c√πng
#   - L∆∞u k·∫øt qu·∫£: df.to_csv('clean_data.csv')
```

---

## üí° H∆∞·ªõng d·∫´n l√†m b√†i

### üìå Quy tr√¨nh l√†m b√†i chu·∫©n:
1. **ƒê·ªçc y√™u c·∫ßu** k·ªπ c√†ng
2. **Ch·∫°y code m·∫´u** ƒë·ªÉ hi·ªÉu d·ªØ li·ªáu g·ªëc
3. **Vi·∫øt code** gi·∫£i quy·∫øt t·ª´ng TODO
4. **Ki·ªÉm tra k·∫øt qu·∫£** v·ªõi `.head()`, `.info()`, `.describe()`
5. **Th√™m comments** gi·∫£i th√≠ch logic

### ‚ö†Ô∏è L∆∞u √Ω quan tr·ªçng:
- Lu√¥n t·∫°o **copy** c·ªßa DataFrame tr∆∞·ªõc khi x·ª≠ l√Ω: `df_clean = df.copy()`
- Ki·ªÉm tra **ki·ªÉu d·ªØ li·ªáu** tr∆∞·ªõc khi √°p d·ª•ng scaler
- V·ªõi **One-Hot Encoding**, c·∫©n th·∫≠n v·ªõi s·ªë l∆∞·ª£ng categories
- **L∆∞u k·∫øt qu·∫£** ƒë·ªÉ c√≥ th·ªÉ quay l·∫°i ki·ªÉm tra

---

## üéØ M·ª•c ti√™u h·ªçc t·∫≠p

Sau khi ho√†n th√†nh c√°c b√†i t·∫≠p, b·∫°n s·∫Ω c√≥ th·ªÉ:
- ‚úÖ X·ª≠ l√Ω missing data trong d·ªØ li·ªáu th·ª±c t·∫ø
- ‚úÖ L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu
- ‚úÖ Chu·∫©n b·ªã d·ªØ li·ªáu cho machine learning
- ‚úÖ √Åp d·ª•ng c√°c k·ªπ thu·∫≠t v√†o d·ª± √°n th·ª±c t·∫ø
- ‚úÖ T·ª± tin x·ª≠ l√Ω data cleaning pipeline ho√†n ch·ªânh

---

## üìö T√†i li·ªáu tham kh·∫£o

### üìñ Documentation ch√≠nh th·ª©c:
- **Pandas**: https://pandas.pydata.org/docs/
- **Scikit-learn Preprocessing**: https://scikit-learn.org/stable/modules/preprocessing.html
- **Regular Expressions**: https://docs.python.org/3/library/re.html

### üéì T√†i li·ªáu b·ªï sung:
- **Missing Data Guide**: https://pandas.pydata.org/docs/user_guide/missing_data.html
- **Data Cleaning Tutorial**: https://realpython.com/python-data-cleaning-numpy-pandas/

---

**Ch√∫c b·∫°n h·ªçc t·∫≠p hi·ªáu qu·∫£! üöÄ**

**L∆∞u √Ω**: H√£y n·ªôp B√†i 6 (D·ª± √°n t·ªïng h·ª£p) qua Google Drive ƒë·ªÉ ƒë∆∞·ª£c ch·∫•m ƒëi·ªÉm!



